[
["index.html", "ANN in R Chapter 1 Introduction", " ANN in R Dr Sebnem Er 2021-05-20 Chapter 1 Introduction In this tutorial you will be introduced to several R packages and how to use some of them such as neuralnet , keras and h2o . In any of these packages the order of NN application is standard: 1 - Preprocess your data: Standardize or normalize your data (scale or min-max) Missing values, outliers Training/Validation and Test set splits 2 - Construct your NN model, ie. number of neurons, number of layers, learning rate, regularization etc. (these are your hyperparameters) 3 - Fit your model 4 - Predict your Y variable for both training and validation sets. 5 - Extract the training and validation errors, visualize these for different number of hyperparameters (fine tuning) 6 - Choose the parameters based on the smallest validation error. 7 - Assess the performance of your predicted model on the test set. Remember the test set error is not for decision making! "],
["neuralnet.html", "Chapter 2 Neuralnet Package 2.1 Example 1 - Boston 2.2 Example 2 - Iris Dataset", " Chapter 2 Neuralnet Package #install.packages(&quot;neuralnet&quot;) rm(list=ls()) library(neuralnet) 2.1 Example 1 - Boston library(MASS) rm(list=ls()) data(Boston) 2.1.1 Scale, Training and Test Datasets Boston.st = scale(Boston) set.seed(1) index &lt;- sample(1:nrow(Boston.st),round(0.8*nrow(Boston.st))) trainBoston.st &lt;- Boston.st[index,] testBoston.st &lt;- Boston.st[-index,] 2.1.2 Build model set.seed(1) neuralnetmodel = neuralnet(medv~crim+nox, data = trainBoston.st, linear.output = TRUE, act.fct = &quot;logistic&quot;, hidden = c(2)) plot(neuralnetmodel) neuralnetmodel$weights ## [[1]] ## [[1]][[1]] ## [,1] [,2] ## [1,] -38.34128 5.724901 ## [2,] 20.11112 -3.802220 ## [3,] -122.96637 -2.323184 ## ## [[1]][[2]] ## [,1] ## [1,] -1.4262287 ## [2,] 0.6035351 ## [3,] 1.3660896 neuralnetmodel$err.fct ## function (x, y) ## { ## 1/2 * (y - x)^2 ## } ## &lt;bytecode: 0x000000001c462b10&gt; ## &lt;environment: 0x000000001c461488&gt; ## attr(,&quot;type&quot;) ## [1] &quot;sse&quot; neuralnetmodel$act.fct ## function (x) ## { ## 1/(1 + exp(-x)) ## } ## &lt;bytecode: 0x000000001c45c288&gt; ## &lt;environment: 0x000000001c45b990&gt; ## attr(,&quot;type&quot;) ## [1] &quot;logistic&quot; 2.1.3 Get the predictions train set train.st.y.predict = predict(neuralnetmodel,trainBoston.st) head(train.st.y.predict, 3) ## [,1] ## 505 -0.06150515 ## 324 0.54309902 ## 167 -0.06612987 1/2*(sum((trainBoston.st[,&quot;medv&quot;]-as.numeric(train.st.y.predict))^2)) ## [1] 153.874 #[1] 153.874 average.error.train = 1/(405)*(sum((trainBoston.st[,&quot;medv&quot;]-as.numeric(train.st.y.predict))^2)) average.error.train ## [1] 0.7598714 153.874*2/405 ## [1] 0.7598716 2.1.4 Get the predictions test set test.st.y.predict = predict(neuralnetmodel,testBoston.st) head(test.st.y.predict, 3) ## [,1] ## 6 0.54326440 ## 7 -0.06064561 ## 9 -0.06067375 1/2*(sum((testBoston.st[,&quot;medv&quot;]-as.numeric(test.st.y.predict))^2)) ## [1] 22.56024 #[1] 22.56024 average.error.test = 1/(101)*(sum((testBoston.st[,&quot;medv&quot;]-as.numeric(test.st.y.predict))^2)) average.error.test ## [1] 0.4467374 2.1.5 Forward propagation head(trainBoston.st[,c(&quot;crim&quot;,&quot;nox&quot;,&quot;medv&quot;)]) ## crim nox medv ## 505 -0.40736095 0.1579678 -0.05793197 ## 324 -0.38709366 -0.5324154 -0.43848654 ## 167 -0.18640065 0.4341211 2.98650460 ## 129 -0.38226778 0.5980871 -0.49285148 ## 418 2.59570533 1.0727255 -1.31919854 ## 471 0.08548074 0.2183763 -0.28626471 trainBoston.st[1,c(&quot;crim&quot;,&quot;nox&quot;,&quot;medv&quot;)] ## crim nox medv ## -0.40736095 0.15796779 -0.05793197 Z1_2 = sum(neuralnetmodel$weights[[1]][[1]][,1]*c(1,trainBoston.st[1,c(&quot;crim&quot;,&quot;nox&quot;)])) Z1_2 ## [1] -65.95849 a1_2 = 1/(1+exp(-Z1_2)) a1_2 ## [1] 2.26251e-29 Z2_2 = sum(neuralnetmodel$weights[[1]][[1]][,2]*c(1,trainBoston.st[1,c(&quot;crim&quot;,&quot;nox&quot;)])) Z2_2 ## [1] 6.906789 a2_2 = 1/(1+exp(-Z2_2)) a2_2 ## [1] 0.999 Z1_3 = sum(neuralnetmodel$weights[[1]][[2]][,1]*c(1,a1_2,a2_2)) a1_3=Z1_3 a1_3 ## [1] -0.06150515 2.1.6 Model Tuning Although these seem like good results this may simply be a result of the subseted training and testing data so it is important to test the model performance further. In this example I will perform k-fold cross validation using 10 folds (10 fold cross validation) get the validation indeces using the createFolds function provided by the caret package #install.packages(&quot;caret&quot;) library(caret) ## Loading required package: lattice ## Loading required package: ggplot2 ## Warning: replacing previous import &#39;vctrs::data_frame&#39; by &#39;tibble::data_frame&#39; ## when loading &#39;dplyr&#39; set.seed(1) folds &lt;- createFolds(trainBoston.st[,&quot;medv&quot;], k = 10) #results is a vector that will contain the average sum error square for each # of the network trainings for the validation set. #errors.cv.number.of.neurons &lt;- c() #for (hidden in 1:2){ errors.cv.validation &lt;- c() for (fld in folds){ #train the network (note I have subsetted out the indeces in the validation set) set.seed(1) neuralnetmodel &lt;- neuralnet(medv~crim+nox, data = trainBoston.st[-fld,], linear.output = TRUE, act.fct = &quot;logistic&quot;, hidden = c(2)) #get the predictions from the network for the validation set yhat.fld &lt;- predict(neuralnetmodel, trainBoston.st[fld,]) errorsq.fld = (trainBoston.st[fld,&quot;medv&quot;]- as.numeric(yhat.fld))^2 errors.cv.validation &lt;- c(errors.cv.validation, errorsq.fld) } #errors.cv.number.of.neurons[hidden] = sum(errors.cv.validation)/dim(trainBoston.st)[1] ## [1] 0.7712774 2.2 Example 2 - Iris Dataset rm(list=ls()) set.seed(1) index &lt;- sample(1:nrow(iris),round(0.8*nrow(iris))) trainiris &lt;- iris[index,] testiris &lt;- iris[-index,] set.seed(1) neuralnetmodel = neuralnet(Species~Petal.Length+Petal.Width, data = trainiris, linear.output = FALSE, act.fct = &quot;logistic&quot;, hidden = c(2), err.fct = &quot;ce&quot;) # stepmax = 1000000, threshold = 0.001, rep = 10 values can be changed as well. plot(neuralnetmodel) neuralnetmodel$weights ## [[1]] ## [[1]][[1]] ## [,1] [,2] ## [1,] 9.189152 -124.80564 ## [2,] -1.436100 33.13635 ## [3,] -1.300787 52.19249 ## ## [[1]][[2]] ## [,1] [,2] [,3] ## [1,] -2.83673 -38.59970 4.022241 ## [2,] 39.67766 28.62683 -28.834230 ## [3,] -113.70312 24.75467 9.921314 neuralnetmodel$err.fct ## function (x, y) ## { ## -(y * log(x) + (1 - y) * log(1 - x)) ## } ## &lt;bytecode: 0x000000001c4622f8&gt; ## &lt;environment: 0x0000000024577c00&gt; ## attr(,&quot;type&quot;) ## [1] &quot;ce&quot; neuralnetmodel$act.fct ## function (x) ## { ## 1/(1 + exp(-x)) ## } ## &lt;bytecode: 0x000000001c45c288&gt; ## &lt;environment: 0x00000000245ce3f8&gt; ## attr(,&quot;type&quot;) ## [1] &quot;logistic&quot; train.iris.predict = predict(neuralnetmodel, trainiris) head(round(train.iris.predict,3), 3) ## [,1] [,2] [,3] ## 68 0 1 0 ## 129 0 0 1 ## 43 1 0 0 maxprobability &lt;- apply(train.iris.predict, 1, which.max) train.iris.predict.class &lt;- c(&#39;setosa&#39;, &#39;versicolor&#39;, &#39;virginica&#39;)[maxprobability] head(train.iris.predict.class) ## [1] &quot;versicolor&quot; &quot;virginica&quot; &quot;setosa&quot; &quot;setosa&quot; &quot;versicolor&quot; ## [6] &quot;versicolor&quot; table(train.iris.predict.class, trainiris$Species) ## ## train.iris.predict.class setosa versicolor virginica ## setosa 39 0 0 ## versicolor 0 36 2 ## virginica 0 2 41 mean(train.iris.predict.class== trainiris$Species)*100 ## [1] 96.66667 test.iris.predict = predict(neuralnetmodel, testiris) head(round(train.iris.predict,3), 3) ## [,1] [,2] [,3] ## 68 0 1 0 ## 129 0 0 1 ## 43 1 0 0 maxprobability &lt;- apply(test.iris.predict, 1, which.max) test.iris.predict.class &lt;- c(&#39;setosa&#39;, &#39;versicolor&#39;, &#39;virginica&#39;)[maxprobability] head(test.iris.predict.class) ## [1] &quot;setosa&quot; &quot;setosa&quot; &quot;setosa&quot; &quot;setosa&quot; &quot;setosa&quot; &quot;setosa&quot; table(test.iris.predict.class, testiris$Species) ## ## test.iris.predict.class setosa versicolor virginica ## setosa 11 0 0 ## versicolor 0 12 2 ## virginica 0 0 5 mean(test.iris.predict.class== testiris$Species)*100 ## [1] 93.33333 2.2.1 Model Tuning Get the validation indices using the createFolds function provided by the caret package set.seed(1) folds &lt;- createFolds(iris$Species, k = 10) #results is a vector that will contain the accuracy for each of the network trainings and testing results &lt;- c() for (fld in folds){ #train the network set.seed(1) nn &lt;- neuralnet(Species~Petal.Length+Petal.Width, data = iris[-fld,], hidden = c(2), err.fct = &quot;ce&quot;, act.fct = &quot;logistic&quot;, linear.output = FALSE, stepmax = 1000000) #get the classifications from the network classes &lt;- predict(nn, iris[fld,]) maxprobability &lt;- apply(classes, 1, which.max) valid.iris.predict.class &lt;- c(&#39;setosa&#39;, &#39;versicolor&#39;, &#39;virginica&#39;)[maxprobability] results &lt;- c(results, valid.iris.predict.class== iris[fld,&quot;Species&quot;]) } results ## [1] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE ## [13] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE ## [25] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE ## [37] TRUE TRUE TRUE TRUE FALSE TRUE TRUE TRUE TRUE TRUE TRUE TRUE ## [49] TRUE TRUE TRUE FALSE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE ## [61] TRUE TRUE TRUE TRUE TRUE TRUE FALSE TRUE TRUE TRUE TRUE TRUE ## [73] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE ## [85] TRUE TRUE TRUE TRUE FALSE TRUE TRUE TRUE TRUE TRUE TRUE TRUE ## [97] TRUE TRUE FALSE TRUE TRUE TRUE FALSE TRUE TRUE TRUE TRUE TRUE ## [109] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE ## [121] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE ## [133] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE ## [145] TRUE TRUE TRUE TRUE TRUE TRUE sum(results)/length(results) ## [1] 0.96 You can label chapter and section titles using {#label} after them, e.g., we can reference Chapter 2. If you do not manually label them, there will be automatic labels anyway, e.g., Chapter 4. Figures and tables with captions will be placed in figure and table environments, respectively. par(mar = c(4, 4, .1, .1)) plot(pressure, type = &#39;b&#39;, pch = 19) Figure 2.1: Here is a nice figure! Reference a figure by its code chunk label with the fig: prefix, e.g., see Figure 2.1. Similarly, you can reference tables generated from knitr::kable(), e.g., see Table 2.1. knitr::kable( head(iris, 20), caption = &#39;Here is a nice table!&#39;, booktabs = TRUE ) Table 2.1: Here is a nice table! Sepal.Length Sepal.Width Petal.Length Petal.Width Species 5.1 3.5 1.4 0.2 setosa 4.9 3.0 1.4 0.2 setosa 4.7 3.2 1.3 0.2 setosa 4.6 3.1 1.5 0.2 setosa 5.0 3.6 1.4 0.2 setosa 5.4 3.9 1.7 0.4 setosa 4.6 3.4 1.4 0.3 setosa 5.0 3.4 1.5 0.2 setosa 4.4 2.9 1.4 0.2 setosa 4.9 3.1 1.5 0.1 setosa 5.4 3.7 1.5 0.2 setosa 4.8 3.4 1.6 0.2 setosa 4.8 3.0 1.4 0.1 setosa 4.3 3.0 1.1 0.1 setosa 5.8 4.0 1.2 0.2 setosa 5.7 4.4 1.5 0.4 setosa 5.4 3.9 1.3 0.4 setosa 5.1 3.5 1.4 0.3 setosa 5.7 3.8 1.7 0.3 setosa 5.1 3.8 1.5 0.3 setosa You can write citations, too. For example, we are using the bookdown package (Xie 2020) in this sample book, which was built on top of R Markdown and knitr (Xie 2015). References "],
["h2o.html", "Chapter 3 H2O 3.1 Initialise H2O Connection 3.2 Data in H2o format", " Chapter 3 H2O H2O is probably the easiest to learn and use. ## Load all packages first library(h2o) ## Warning: package &#39;h2o&#39; was built under R version 4.0.5 ## ## ---------------------------------------------------------------------- ## ## Your next step is to start H2O: ## &gt; h2o.init() ## ## For H2O package documentation, ask for help: ## &gt; ??h2o ## ## After starting H2O, you can use the Web UI at http://localhost:54321 ## For more information visit https://docs.h2o.ai ## ## ---------------------------------------------------------------------- ## ## Attaching package: &#39;h2o&#39; ## The following objects are masked from &#39;package:stats&#39;: ## ## cor, sd, var ## The following objects are masked from &#39;package:base&#39;: ## ## %*%, %in%, &amp;&amp;, ||, apply, as.factor, as.numeric, colnames, ## colnames&lt;-, ifelse, is.character, is.factor, is.numeric, log, ## log10, log1p, log2, round, signif, trunc library(caret) library(mlbench) library(ggplot2) library(reshape2) library(DEEPR) ## Warning: package &#39;DEEPR&#39; was built under R version 4.0.3 ## Loading required package: dirmult ## Warning: package &#39;dirmult&#39; was built under R version 4.0.3 # http://blog.revolutionanalytics.com/2014/04/a-dive-into-h2o.html # https://discuss.analyticsvidhya.com/t/script-in-h2o-in-r-to-get-you-into-top-30-percentile-for-the-digit-recognizer-competition/6651/8 3.1 Initialise H2O Connection ## Start a local H2O cluster directly from R localH2O = h2o.init(ip = &quot;localhost&quot;, port = 54321, startH2O = TRUE,min_mem_size = &quot;3g&quot;) ## Connection successful! ## ## R is connected to the H2O cluster: ## H2O cluster uptime: 55 minutes 35 seconds ## H2O cluster timezone: Africa/Johannesburg ## H2O data parsing timezone: UTC ## H2O cluster version: 3.32.0.1 ## H2O cluster version age: 7 months and 11 days !!! ## H2O cluster name: H2O_started_from_R_01438475_sup231 ## H2O cluster total nodes: 1 ## H2O cluster total memory: 3.90 GB ## H2O cluster total cores: 4 ## H2O cluster allowed cores: 4 ## H2O cluster healthy: TRUE ## H2O Connection ip: localhost ## H2O Connection port: 54321 ## H2O Connection proxy: NA ## H2O Internal Security: FALSE ## H2O API Extensions: Amazon S3, Algos, AutoML, Core V3, TargetEncoder, Core V4 ## R Version: R version 4.0.1 (2020-06-06) ## Warning in h2o.clusterInfo(): ## Your H2O cluster version is too old (7 months and 11 days)! ## Please download and install the latest version from http://h2o.ai/download/ #Get help ?h2o.deeplearning ## starting httpd help server ... done 3.2 Data in H2o format # iris data #### iris ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3.0 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5.0 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa ## 7 4.6 3.4 1.4 0.3 setosa ## 8 5.0 3.4 1.5 0.2 setosa ## 9 4.4 2.9 1.4 0.2 setosa ## 10 4.9 3.1 1.5 0.1 setosa ## 11 5.4 3.7 1.5 0.2 setosa ## 12 4.8 3.4 1.6 0.2 setosa ## 13 4.8 3.0 1.4 0.1 setosa ## 14 4.3 3.0 1.1 0.1 setosa ## 15 5.8 4.0 1.2 0.2 setosa ## 16 5.7 4.4 1.5 0.4 setosa ## 17 5.4 3.9 1.3 0.4 setosa ## 18 5.1 3.5 1.4 0.3 setosa ## 19 5.7 3.8 1.7 0.3 setosa ## 20 5.1 3.8 1.5 0.3 setosa ## 21 5.4 3.4 1.7 0.2 setosa ## 22 5.1 3.7 1.5 0.4 setosa ## 23 4.6 3.6 1.0 0.2 setosa ## 24 5.1 3.3 1.7 0.5 setosa ## 25 4.8 3.4 1.9 0.2 setosa ## 26 5.0 3.0 1.6 0.2 setosa ## 27 5.0 3.4 1.6 0.4 setosa ## 28 5.2 3.5 1.5 0.2 setosa ## 29 5.2 3.4 1.4 0.2 setosa ## 30 4.7 3.2 1.6 0.2 setosa ## 31 4.8 3.1 1.6 0.2 setosa ## 32 5.4 3.4 1.5 0.4 setosa ## 33 5.2 4.1 1.5 0.1 setosa ## 34 5.5 4.2 1.4 0.2 setosa ## 35 4.9 3.1 1.5 0.2 setosa ## 36 5.0 3.2 1.2 0.2 setosa ## 37 5.5 3.5 1.3 0.2 setosa ## 38 4.9 3.6 1.4 0.1 setosa ## 39 4.4 3.0 1.3 0.2 setosa ## 40 5.1 3.4 1.5 0.2 setosa ## 41 5.0 3.5 1.3 0.3 setosa ## 42 4.5 2.3 1.3 0.3 setosa ## 43 4.4 3.2 1.3 0.2 setosa ## 44 5.0 3.5 1.6 0.6 setosa ## 45 5.1 3.8 1.9 0.4 setosa ## 46 4.8 3.0 1.4 0.3 setosa ## 47 5.1 3.8 1.6 0.2 setosa ## 48 4.6 3.2 1.4 0.2 setosa ## 49 5.3 3.7 1.5 0.2 setosa ## 50 5.0 3.3 1.4 0.2 setosa ## 51 7.0 3.2 4.7 1.4 versicolor ## 52 6.4 3.2 4.5 1.5 versicolor ## 53 6.9 3.1 4.9 1.5 versicolor ## 54 5.5 2.3 4.0 1.3 versicolor ## 55 6.5 2.8 4.6 1.5 versicolor ## 56 5.7 2.8 4.5 1.3 versicolor ## 57 6.3 3.3 4.7 1.6 versicolor ## 58 4.9 2.4 3.3 1.0 versicolor ## 59 6.6 2.9 4.6 1.3 versicolor ## 60 5.2 2.7 3.9 1.4 versicolor ## 61 5.0 2.0 3.5 1.0 versicolor ## 62 5.9 3.0 4.2 1.5 versicolor ## 63 6.0 2.2 4.0 1.0 versicolor ## 64 6.1 2.9 4.7 1.4 versicolor ## 65 5.6 2.9 3.6 1.3 versicolor ## 66 6.7 3.1 4.4 1.4 versicolor ## 67 5.6 3.0 4.5 1.5 versicolor ## 68 5.8 2.7 4.1 1.0 versicolor ## 69 6.2 2.2 4.5 1.5 versicolor ## 70 5.6 2.5 3.9 1.1 versicolor ## 71 5.9 3.2 4.8 1.8 versicolor ## 72 6.1 2.8 4.0 1.3 versicolor ## 73 6.3 2.5 4.9 1.5 versicolor ## 74 6.1 2.8 4.7 1.2 versicolor ## 75 6.4 2.9 4.3 1.3 versicolor ## 76 6.6 3.0 4.4 1.4 versicolor ## 77 6.8 2.8 4.8 1.4 versicolor ## 78 6.7 3.0 5.0 1.7 versicolor ## 79 6.0 2.9 4.5 1.5 versicolor ## 80 5.7 2.6 3.5 1.0 versicolor ## 81 5.5 2.4 3.8 1.1 versicolor ## 82 5.5 2.4 3.7 1.0 versicolor ## 83 5.8 2.7 3.9 1.2 versicolor ## 84 6.0 2.7 5.1 1.6 versicolor ## 85 5.4 3.0 4.5 1.5 versicolor ## 86 6.0 3.4 4.5 1.6 versicolor ## 87 6.7 3.1 4.7 1.5 versicolor ## 88 6.3 2.3 4.4 1.3 versicolor ## 89 5.6 3.0 4.1 1.3 versicolor ## 90 5.5 2.5 4.0 1.3 versicolor ## 91 5.5 2.6 4.4 1.2 versicolor ## 92 6.1 3.0 4.6 1.4 versicolor ## 93 5.8 2.6 4.0 1.2 versicolor ## 94 5.0 2.3 3.3 1.0 versicolor ## 95 5.6 2.7 4.2 1.3 versicolor ## 96 5.7 3.0 4.2 1.2 versicolor ## 97 5.7 2.9 4.2 1.3 versicolor ## 98 6.2 2.9 4.3 1.3 versicolor ## 99 5.1 2.5 3.0 1.1 versicolor ## 100 5.7 2.8 4.1 1.3 versicolor ## 101 6.3 3.3 6.0 2.5 virginica ## 102 5.8 2.7 5.1 1.9 virginica ## 103 7.1 3.0 5.9 2.1 virginica ## 104 6.3 2.9 5.6 1.8 virginica ## 105 6.5 3.0 5.8 2.2 virginica ## 106 7.6 3.0 6.6 2.1 virginica ## 107 4.9 2.5 4.5 1.7 virginica ## 108 7.3 2.9 6.3 1.8 virginica ## 109 6.7 2.5 5.8 1.8 virginica ## 110 7.2 3.6 6.1 2.5 virginica ## 111 6.5 3.2 5.1 2.0 virginica ## 112 6.4 2.7 5.3 1.9 virginica ## 113 6.8 3.0 5.5 2.1 virginica ## 114 5.7 2.5 5.0 2.0 virginica ## 115 5.8 2.8 5.1 2.4 virginica ## 116 6.4 3.2 5.3 2.3 virginica ## 117 6.5 3.0 5.5 1.8 virginica ## 118 7.7 3.8 6.7 2.2 virginica ## 119 7.7 2.6 6.9 2.3 virginica ## 120 6.0 2.2 5.0 1.5 virginica ## 121 6.9 3.2 5.7 2.3 virginica ## 122 5.6 2.8 4.9 2.0 virginica ## 123 7.7 2.8 6.7 2.0 virginica ## 124 6.3 2.7 4.9 1.8 virginica ## 125 6.7 3.3 5.7 2.1 virginica ## 126 7.2 3.2 6.0 1.8 virginica ## 127 6.2 2.8 4.8 1.8 virginica ## 128 6.1 3.0 4.9 1.8 virginica ## 129 6.4 2.8 5.6 2.1 virginica ## 130 7.2 3.0 5.8 1.6 virginica ## 131 7.4 2.8 6.1 1.9 virginica ## 132 7.9 3.8 6.4 2.0 virginica ## 133 6.4 2.8 5.6 2.2 virginica ## 134 6.3 2.8 5.1 1.5 virginica ## 135 6.1 2.6 5.6 1.4 virginica ## 136 7.7 3.0 6.1 2.3 virginica ## 137 6.3 3.4 5.6 2.4 virginica ## 138 6.4 3.1 5.5 1.8 virginica ## 139 6.0 3.0 4.8 1.8 virginica ## 140 6.9 3.1 5.4 2.1 virginica ## 141 6.7 3.1 5.6 2.4 virginica ## 142 6.9 3.1 5.1 2.3 virginica ## 143 5.8 2.7 5.1 1.9 virginica ## 144 6.8 3.2 5.9 2.3 virginica ## 145 6.7 3.3 5.7 2.5 virginica ## 146 6.7 3.0 5.2 2.3 virginica ## 147 6.3 2.5 5.0 1.9 virginica ## 148 6.5 3.0 5.2 2.0 virginica ## 149 6.2 3.4 5.4 2.3 virginica ## 150 5.9 3.0 5.1 1.8 virginica index &lt;- c(sample(1:50,25), sample(51:100,25), sample(101:150,25)) irisTrain = iris[index,] irisTest = iris[-index,] iris.h2oTrain &lt;- as.h2o(irisTrain) ## Warning in use.package(&quot;data.table&quot;): data.table cannot be used without R ## package bit64 version 0.9.7 or higher. Please upgrade to take advangage of ## data.table speedups. ## | | | 0% | |======================================================================| 100% iris.h2oTest &lt;- as.h2o(irisTest) ## Warning in use.package(&quot;data.table&quot;): data.table cannot be used without R ## package bit64 version 0.9.7 or higher. Please upgrade to take advangage of ## data.table speedups. ## | | | 0% | |======================================================================| 100% iris.nn &lt;- h2o.deeplearning(x = 1:4 , y = 5, training_frame = iris.h2oTrain, # data in H2O format validation_frame = iris.h2oTest, activation = &quot;Tanh&quot;, hidden = c(5), # one layer of 5 nodes l1 = 1e-5, epochs = 100, variable_importances = TRUE) # max. no. of epochs ## | | | 0% | |======================================================================| 100% iris.nn.cv &lt;- h2o.deeplearning(x = 1:4 , y = 5, training_frame = iris.h2oTrain, # data in H2O format validation_frame = iris.h2oTest, activation = &quot;Tanh&quot;, hidden = c(5), # one layer of 5 nodes l1 = 1e-5, nfolds = 5, epochs = 100) # max. no. of epochs ## | | | 0% | |============================================================== | 88% | |======================================================================| 100% iris.nn@parameters ## $model_id ## [1] &quot;DeepLearning_model_R_1621504187964_6&quot; ## ## $training_frame ## [1] &quot;irisTrain_sid_a943_1&quot; ## ## $validation_frame ## [1] &quot;irisTest_sid_a943_3&quot; ## ## $activation ## [1] &quot;Tanh&quot; ## ## $hidden ## [1] 5 ## ## $epochs ## [1] 100 ## ## $seed ## [1] &quot;140125080671732877&quot; ## ## $l1 ## [1] 1e-05 ## ## $distribution ## [1] &quot;multinomial&quot; ## ## $stopping_metric ## [1] &quot;logloss&quot; ## ## $categorical_encoding ## [1] &quot;OneHotInternal&quot; ## ## $x ## [1] &quot;Sepal.Length&quot; &quot;Sepal.Width&quot; &quot;Petal.Length&quot; &quot;Petal.Width&quot; ## ## $y ## [1] &quot;Species&quot; h2o.performance(iris.nn, train = TRUE) ## H2OMultinomialMetrics: deeplearning ## ** Reported on training data. ** ## ** Metrics reported on full training frame ** ## ## Training Set Metrics: ## ===================== ## ## Extract training frame with `h2o.getFrame(&quot;irisTrain_sid_a943_1&quot;)` ## MSE: (Extract with `h2o.mse`) 0.04022331 ## RMSE: (Extract with `h2o.rmse`) 0.2005575 ## Logloss: (Extract with `h2o.logloss`) 0.1443866 ## Mean Per-Class Error: 0.04 ## Confusion Matrix: Extract with `h2o.confusionMatrix(&lt;model&gt;,train = TRUE)`) ## ========================================================================= ## Confusion Matrix: Row labels: Actual class; Column labels: Predicted class ## setosa versicolor virginica Error Rate ## setosa 25 0 0 0.0000 = 0 / 25 ## versicolor 0 25 0 0.0000 = 0 / 25 ## virginica 0 3 22 0.1200 = 3 / 25 ## Totals 25 28 22 0.0400 = 3 / 75 ## ## Hit Ratio Table: Extract with `h2o.hit_ratio_table(&lt;model&gt;,train = TRUE)` ## ======================================================================= ## Top-3 Hit Ratios: ## k hit_ratio ## 1 1 0.960000 ## 2 2 1.000000 ## 3 3 1.000000 h2o.mse(iris.nn, train = TRUE) ## [1] 0.04022331 h2o.mse(iris.nn, valid = TRUE) ## [1] 0.03606636 h2o.mse(iris.nn.cv, xval = TRUE) ## [1] 0.08550383 h2o.varimp(iris.nn) ## Variable Importances: ## variable relative_importance scaled_importance percentage ## 1 Petal.Length 1.000000 1.000000 0.404590 ## 2 Petal.Width 0.579513 0.579513 0.234465 ## 3 Sepal.Length 0.559525 0.559525 0.226378 ## 4 Sepal.Width 0.332603 0.332603 0.134568 # now make a prediction predictionsTrain &lt;- h2o.predict(iris.nn, iris.h2oTrain) ## | | | 0% | |======================================================================| 100% predictionsTrain ## predict setosa versicolor virginica ## 1 setosa 0.9605705 0.039408090 2.136172e-05 ## 2 setosa 0.9941566 0.005842733 6.915862e-07 ## 3 setosa 0.9932530 0.006746304 7.382293e-07 ## 4 setosa 0.9938170 0.006182201 7.907678e-07 ## 5 setosa 0.9950551 0.004944093 8.316701e-07 ## 6 setosa 0.9937289 0.006270050 1.058508e-06 ## ## [75 rows x 4 columns] predictionsTest &lt;- h2o.predict(iris.nn, iris.h2oTest) ## | | | 0% | |======================================================================| 100% predictionsTest ## predict setosa versicolor virginica ## 1 setosa 0.9947001 0.005298876 9.823197e-07 ## 2 setosa 0.9930255 0.006973262 1.263730e-06 ## 3 setosa 0.9945754 0.005423825 7.557000e-07 ## 4 setosa 0.9932355 0.006763449 1.040563e-06 ## 5 setosa 0.9925999 0.007398490 1.571062e-06 ## 6 setosa 0.9941181 0.005880774 1.130556e-06 ## ## [75 rows x 4 columns] yhatTrain = as.factor(as.matrix(predictionsTrain$predict)) confusionMatrix(yhatTrain, irisTrain$Species) ## Confusion Matrix and Statistics ## ## Reference ## Prediction setosa versicolor virginica ## setosa 25 0 0 ## versicolor 0 25 3 ## virginica 0 0 22 ## ## Overall Statistics ## ## Accuracy : 0.96 ## 95% CI : (0.8875, 0.9917) ## No Information Rate : 0.3333 ## P-Value [Acc &gt; NIR] : &lt; 2.2e-16 ## ## Kappa : 0.94 ## ## Mcnemar&#39;s Test P-Value : NA ## ## Statistics by Class: ## ## Class: setosa Class: versicolor Class: virginica ## Sensitivity 1.0000 1.0000 0.8800 ## Specificity 1.0000 0.9400 1.0000 ## Pos Pred Value 1.0000 0.8929 1.0000 ## Neg Pred Value 1.0000 1.0000 0.9434 ## Prevalence 0.3333 0.3333 0.3333 ## Detection Rate 0.3333 0.3333 0.2933 ## Detection Prevalence 0.3333 0.3733 0.2933 ## Balanced Accuracy 1.0000 0.9700 0.9400 yhatTest = as.factor(as.matrix(predictionsTest$predict)) confusionMatrix(yhatTest, irisTest$Species) ## Confusion Matrix and Statistics ## ## Reference ## Prediction setosa versicolor virginica ## setosa 25 0 0 ## versicolor 0 25 3 ## virginica 0 0 22 ## ## Overall Statistics ## ## Accuracy : 0.96 ## 95% CI : (0.8875, 0.9917) ## No Information Rate : 0.3333 ## P-Value [Acc &gt; NIR] : &lt; 2.2e-16 ## ## Kappa : 0.94 ## ## Mcnemar&#39;s Test P-Value : NA ## ## Statistics by Class: ## ## Class: setosa Class: versicolor Class: virginica ## Sensitivity 1.0000 1.0000 0.8800 ## Specificity 1.0000 0.9400 1.0000 ## Pos Pred Value 1.0000 0.8929 1.0000 ## Neg Pred Value 1.0000 1.0000 0.9434 ## Prevalence 0.3333 0.3333 0.3333 ## Detection Rate 0.3333 0.3333 0.2933 ## Detection Prevalence 0.3333 0.3733 0.2933 ## Balanced Accuracy 1.0000 0.9700 0.9400 3.2.1 Grid Search for Complexity https://h2o-release.s3.amazonaws.com/h2o/master/3190/docs-website/h2o-docs/booklets/DeepLearning_Vignette.pdf hidden_opt &lt;- list(c(1), c(2), c(3), 4,5,6,7,8,9,10, c(3,4),c(4,4), c(5,4), c(6,4)) hyper_params &lt;- list(hidden = hidden_opt) model_grid &lt;- h2o.grid(&quot;deeplearning&quot;, hyper_params = hyper_params, x = 1:4 , y = 5, training_frame = iris.h2oTrain, # data in H2O format validation_frame = iris.h2oTest, activation = &quot;Tanh&quot;, seed = 1, reproducible = TRUE, nfolds = 5 ) ## | | | 0% | |======================================================================| 100% model_grid ## H2O Grid Details ## ================ ## ## Grid ID: Grid_DeepLearning_irisTrain_sid_a943_1_model_R_1621504187964_8 ## Used hyper parameters: ## - hidden ## Number of models: 14 ## Number of failed models: 0 ## ## Hyper-Parameter Search Summary: ordered by increasing logloss ## hidden ## 1 [6, 4] ## 2 [10] ## 3 [9] ## 4 [5, 4] ## 5 [8] ## 6 [4, 4] ## 7 [4] ## 8 [5] ## 9 [7] ## 10 [3, 4] ## 11 [1] ## 12 [6] ## 13 [2] ## 14 [3] ## model_ids ## 1 Grid_DeepLearning_irisTrain_sid_a943_1_model_R_1621504187964_8_model_14 ## 2 Grid_DeepLearning_irisTrain_sid_a943_1_model_R_1621504187964_8_model_10 ## 3 Grid_DeepLearning_irisTrain_sid_a943_1_model_R_1621504187964_8_model_9 ## 4 Grid_DeepLearning_irisTrain_sid_a943_1_model_R_1621504187964_8_model_13 ## 5 Grid_DeepLearning_irisTrain_sid_a943_1_model_R_1621504187964_8_model_8 ## 6 Grid_DeepLearning_irisTrain_sid_a943_1_model_R_1621504187964_8_model_12 ## 7 Grid_DeepLearning_irisTrain_sid_a943_1_model_R_1621504187964_8_model_4 ## 8 Grid_DeepLearning_irisTrain_sid_a943_1_model_R_1621504187964_8_model_5 ## 9 Grid_DeepLearning_irisTrain_sid_a943_1_model_R_1621504187964_8_model_7 ## 10 Grid_DeepLearning_irisTrain_sid_a943_1_model_R_1621504187964_8_model_11 ## 11 Grid_DeepLearning_irisTrain_sid_a943_1_model_R_1621504187964_8_model_1 ## 12 Grid_DeepLearning_irisTrain_sid_a943_1_model_R_1621504187964_8_model_6 ## 13 Grid_DeepLearning_irisTrain_sid_a943_1_model_R_1621504187964_8_model_2 ## 14 Grid_DeepLearning_irisTrain_sid_a943_1_model_R_1621504187964_8_model_3 ## logloss ## 1 0.30340859353378963 ## 2 0.3241505862508419 ## 3 0.3762406263763066 ## 4 0.3859649879467963 ## 5 0.41505146300210605 ## 6 0.42160893762650176 ## 7 0.44643795766409294 ## 8 0.5033872528865022 ## 9 0.5453880844019323 ## 10 0.5799886955035108 ## 11 0.5823825581709984 ## 12 0.6108454650879463 ## 13 0.702354413999542 ## 14 0.7400050923793164 model1 = h2o.getModel(model_grid@model_ids[[1]]) h2o.mse(model1, xval = TRUE) ## [1] 0.09175803 "],
["keras.html", "Chapter 4 Keras 4.1 Installing and Calling the Keras Package 4.2 Keras for Regression Problems 4.3 Keras for Classification Problems 4.4 Overfitting", " Chapter 4 Keras Developers: Daniel Falbel (Contributor, copyright holder, maintainer), JJ Allaire (Author, copyright holder), Francois Chollet (Author, copyright holder) etc. https://keras.rstudio.com/ https://keras.rstudio.com/articles/sequential_model.html The keras package uses the pipe operator to connect functions or operations together and the datasets need to be in matrix format. 4.1 Installing and Calling the Keras Package #devtools::install_github(&quot;rstudio/keras&quot;) library(keras) library(tensorflow) ## ## Attaching package: &#39;tensorflow&#39; ## The following object is masked from &#39;package:caret&#39;: ## ## train #install.packages(&quot;devtools&quot;) #require(devtools) #install_github(&quot;rstudio/reticulate&quot;) #install_github(&quot;rstudio/tensorflow&quot;) #install_github(&quot;rstudio/keras&quot;, force = TRUE) # Install TensorFlow #install_tensorflow() library(keras) #install_keras() library(tensorflow) #install_tensorflow() library(reticulate) #reticulate::py_available(initialize = TRUE) #reticulate::py_versions_windows() #reticulate::py_config() #reticulate::py_discover_config(&quot;keras&quot;) 4.2 Keras for Regression Problems 4.2.1 Load the dataset, standardize library(MASS) rm(list=ls()) data(Boston) head(Boston) ## crim zn indus chas nox rm age dis rad tax ptratio black lstat ## 1 0.00632 18 2.31 0 0.538 6.575 65.2 4.0900 1 296 15.3 396.90 4.98 ## 2 0.02731 0 7.07 0 0.469 6.421 78.9 4.9671 2 242 17.8 396.90 9.14 ## 3 0.02729 0 7.07 0 0.469 7.185 61.1 4.9671 2 242 17.8 392.83 4.03 ## 4 0.03237 0 2.18 0 0.458 6.998 45.8 6.0622 3 222 18.7 394.63 2.94 ## 5 0.06905 0 2.18 0 0.458 7.147 54.2 6.0622 3 222 18.7 396.90 5.33 ## 6 0.02985 0 2.18 0 0.458 6.430 58.7 6.0622 3 222 18.7 394.12 5.21 ## medv ## 1 24.0 ## 2 21.6 ## 3 34.7 ## 4 33.4 ## 5 36.2 ## 6 28.7 # Standardize Your dataset Boston.st = scale(Boston) set.seed(1) index &lt;- sample(1:nrow(Boston.st),round(0.8*nrow(Boston.st))) trainBoston.st &lt;- Boston.st[index,] testBoston.st &lt;- Boston.st[-index,] head(testBoston.st) ## crim zn indus chas nox rm ## 6 -0.4166314 -0.48724019 -1.3055857 -0.2723291 -0.8344581 0.2068916 ## 7 -0.4098372 0.04872402 -0.4761823 -0.2723291 -0.2648919 -0.3880270 ## 9 -0.3955433 0.04872402 -0.4761823 -0.2723291 -0.2648919 -0.9302853 ## 10 -0.4003331 0.04872402 -0.4761823 -0.2723291 -0.2648919 -0.3994130 ## 11 -0.3939564 0.04872402 -0.4761823 -0.2723291 -0.2648919 0.1314594 ## 12 -0.4064448 0.04872402 -0.4761823 -0.2723291 -0.2648919 -0.3922967 ## age dis rad tax ptratio black lstat ## 6 -0.35080997 1.0766711 -0.7521778 -1.105022 0.1129203 0.4101651 -1.04229087 ## 7 -0.07015919 0.8384142 -0.5224844 -0.576948 -1.5037485 0.4263763 -0.03123671 ## 9 1.11638970 1.0861216 -0.5224844 -0.576948 -1.5037485 0.3281233 2.41937935 ## 10 0.61548134 1.3283202 -0.5224844 -0.576948 -1.5037485 0.3289995 0.62272769 ## 11 0.91389483 1.2117800 -0.5224844 -0.576948 -1.5037485 0.3926395 1.09184562 ## 12 0.50890509 1.1547920 -0.5224844 -0.576948 -1.5037485 0.4406159 0.08639286 ## medv ## 6 0.67055821 ## 7 0.03992492 ## 9 -0.65594629 ## 10 -0.39499459 ## 11 -0.81904111 ## 12 -0.39499459 # The data needs to be in a matrix format and X variables and Y variables should be provided in two different matrices. Y variable &quot;medv&quot; is given in the 14th column of our dataset x_train.st &lt;- as.matrix(trainBoston.st[,-14]) y_train.st &lt;- as.matrix(trainBoston.st[,14]) x_test.st &lt;- as.matrix(testBoston.st[,-14]) y_test.st &lt;- as.matrix(testBoston.st[,14]) dim(x_train.st) ## [1] 405 13 dim(y_train.st) ## [1] 405 1 dim(x_test.st) ## [1] 101 13 dim(y_test.st) ## [1] 101 1 In fact all this is available in Keras from the dataset_boston_housing() function so you can also use the following, remember you still might need to standardize your dataset: #boston_housing &lt;- dataset_boston_housing() #x_train &lt;- boston_housing$train$x #y_train &lt;- boston_housing$train$y #x_test &lt;- boston_housing$test$x #y_test &lt;- boston_housing$test$y 4.2.2 Initialize a sequential feed forward neural network # Initialize an empty sequential model: model.regression &lt;- keras_model_sequential() 4.2.3 Define the structure of your model: layers and the activation functions Add layers to the model, 1 Input, 1 Hidden and 1 Output Layer Layers are defined within the layer_dense() functions We don’t need to define the Input Layer separately since it gets associated with the 1st Hidden layer and no activation occurs in the Input Layer. Though we do need to specify the Output Layer because we need to define the activation function and the dimension. model.regression %&gt;% layer_dense(units = 3, activation = &#39;relu&#39;, input_shape = c(13)) %&gt;% # 13 X variables + 1 constant = input layer has 14 neurons # The 14 neurons are fully connected to 3 neurons in the hidden layer. # 14*2 = 42 weights are optimized # The activation of the hidden layer neurons layer_dense(units = 1, activation = &#39;relu&#39;) # 3 neurons in the hidden layer + 1 constant = 4 neurons in total # The 4 neurons are fully connected to 1 neuron in the output layer # 4*1 = 4 weights to optimize # the activation of the output layer, since a regression problem, best is either relu, linear # Print a summary of a model summary(model.regression) # in total 46 parameters will be optimized. ## Model: &quot;sequential&quot; ## ________________________________________________________________________________ ## Layer (type) Output Shape Param # ## ================================================================================ ## dense_1 (Dense) (None, 3) 42 ## ________________________________________________________________________________ ## dense (Dense) (None, 1) 4 ## ================================================================================ ## Total params: 46 ## Trainable params: 46 ## Non-trainable params: 0 ## ________________________________________________________________________________ 4.2.4 Define how these weights will be optimized This is where the weights will be optimized by minimizing the “Mean Square Error” which is calculated by \\(\\frac{\\sum_{i=1}^{n}(y-\\hat y)^2}{n}\\). model.regression %&gt;% compile( optimizer = optimizer_rmsprop(lr = 0.002), loss = &#39;mse&#39; ) 4.2.5 Run the model without validation model.regression %&gt;% fit(x_train.st, y_train.st, epochs = 20, batch_size = 32) All this could have been done at once since we are using a pipeline operator. We can then evaluate the performance of the model using the test set which means we need to predict the Y values of the test set using this model: predictions.y.train.st = model.regression %&gt;% predict(x_train.st) mse.train.st = sum((y_train.st - predictions.y.train.st)^2)/dim(x_train.st)[1] mse.train.st ## [1] 0.5032314 predictions.y.test.st = model.regression %&gt;% predict(x_test.st) mse.test.st = sum((y_test.st - predictions.y.test.st)^2)/dim(x_test.st)[1] mse.test.st ## [1] 0.4617062 # same as below: model.regression %&gt;% evaluate(x_train.st, y_train.st, batch_size=32, verbose = 1) ## loss ## 0.5032314 model.regression %&gt;% evaluate(x_test.st, y_test.st, batch_size=32, verbose = 1) ## loss ## 0.4617062 4.2.6 Run the model with validation # Fit the model model.regression %&gt;% fit( x_train.st, y_train.st, epochs = 20, batch_size = 32, validation_split = 0.2 ) 4.3 Keras for Classification Problems For classification problems, the Y variable needs to be converted into dummy variables (one-hot-encoding), and the output layer activation function needs to be logistic function: rm(list=ls()) iris$numericclasses = unclass(iris$Species) set.seed(1) index &lt;- sample(1:nrow(iris),round(0.8*nrow(iris))) trainiris &lt;- iris[index,] testiris &lt;- iris[-index,] # no need to standardize this dataset but we still need to convert to matrices x_iristrain &lt;- as.matrix(trainiris[,c(1:4)]) y_iristrain &lt;- as.matrix(trainiris[,6]) # Convert labels to categorical one-hot encoding y_iristrain.one_hot_labels &lt;- to_categorical(y_iristrain, num_classes = 4) x_iristest &lt;- as.matrix(testiris[,c(1:4)]) y_iristest &lt;- as.matrix(testiris[,6]) # Convert labels to categorical one-hot encoding y_iristest.one_hot_labels &lt;- to_categorical(y_iristest, num_classes = 4) 4.3.1 Initialize and define a sequential feed forward neural network The only difference here is we defined all the components together, and remember the output will have 3 neurons with a softmax or logistic activation function defined. model.classification &lt;- keras_model_sequential() %&gt;% layer_dense(units = 3, activation = &#39;relu&#39;, input_shape = c(4)) %&gt;% layer_dense(units = 3, activation = &#39;softmax&#39;) %&gt;% compile(loss = &#39;categorical_crossentropy&#39;, optimizer = &#39;rmsprop&#39;, metrics = c(&#39;accuracy&#39;)) summary(model.classification) ## Model: &quot;sequential_1&quot; ## ________________________________________________________________________________ ## Layer (type) Output Shape Param # ## ================================================================================ ## dense_3 (Dense) (None, 3) 15 ## ________________________________________________________________________________ ## dense_2 (Dense) (None, 3) 12 ## ================================================================================ ## Total params: 27 ## Trainable params: 27 ## Non-trainable params: 0 ## ________________________________________________________________________________ In total 27 parameters will be optimized. (4variables+1 neurons in the input layer = 5 neurons, connected to a hidden layer that has 3 neurons, in total 15 weights. 3 neurons + 1 bias = 4 neurons in the hidden layer are fully connected to the output layer which has 3 neurons, 4*3 = 12 weights to optimize) 4.3.2 Run the model without validation model.classification %&gt;% fit(x_iristrain, y_iristrain.one_hot_labels[,-1], epochs = 200, batch_size = 32) We can then evaluate the performance of the model using the test set which means we need to predict the Y values of the test set using this model: model.classification %&gt;% evaluate(x_iristrain, y_iristrain.one_hot_labels[,-1], batch_size=32, verbose = 1) ## loss accuracy ## 0.4156227 0.8416666 model.classification %&gt;% evaluate(x_iristest, y_iristest.one_hot_labels[,-1], batch_size=32, verbose = 1) ## loss accuracy ## 0.4307769 0.8666667 4.3.3 Run the model with validation # Fit the model model.classification %&gt;% fit(x_iristrain, y_iristrain.one_hot_labels[,-1], epochs = 20, batch_size = 32, validation_split = 0.2 ) 4.3.4 Evaluate the model model.classification %&gt;% evaluate(x_iristrain, y_iristrain.one_hot_labels[,-1], batch_size=32, verbose = 1) ## loss accuracy ## 0.3950052 0.8583333 model.classification %&gt;% evaluate(x_iristest, y_iristest.one_hot_labels[,-1], batch_size=32, verbose = 1) ## loss accuracy ## 0.4106196 0.8666667 4.3.5 Obtain the predictions: iris.train.predicted &lt;- model.classification %&gt;% predict(x_iristrain) head(iris.train.predicted) ## [,1] [,2] [,3] ## [1,] 0.069433182 0.51280409 0.417762667 ## [2,] 0.001320021 0.28976113 0.708918810 ## [3,] 0.948503733 0.04492656 0.006569748 ## [4,] 0.944541514 0.04795999 0.007498457 ## [5,] 0.035015721 0.53584474 0.429139525 ## [6,] 0.022172937 0.45372736 0.524099648 sum(iris.train.predicted[1,]) ## [1] 0.9999999 maxidx &lt;- function(arr) { return(which(arr == max(arr))) } whichmax.train.predicted &lt;- apply(iris.train.predicted, c(1), maxidx) head(whichmax.train.predicted) ## [1] 2 3 1 1 2 3 prediction.train &lt;- c(&#39;setosa&#39;, &#39;versicolor&#39;, &#39;virginica&#39;)[whichmax.train.predicted] actual.train &lt;- c(&#39;setosa&#39;, &#39;versicolor&#39;, &#39;virginica&#39;)[trainiris$Species] table(prediction.train, actual.train) ## actual.train ## prediction.train setosa versicolor virginica ## setosa 39 0 0 ## versicolor 0 21 0 ## virginica 0 17 43 accuracy.train = mean(prediction.train==actual.train) accuracy.train ## [1] 0.8583333 4.4 Overfitting https://tensorflow.rstudio.com/tutorials/beginners/basic-ml/tutorial_overfit_underfit/ "],
["references.html", "References", " References "]
]
