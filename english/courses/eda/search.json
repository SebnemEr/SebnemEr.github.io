[
  {
    "objectID": "lecturecontent.html",
    "href": "lecturecontent.html",
    "title": "STA5092Z EDA Lecture Content",
    "section": "",
    "text": "Lecture\nMaterial\nPossible resources\n\n\n\n\n1-2 [html], [slides], [pdf]\nR installation, basics, workflows\nJB2\n\n\n\nVisualizing raw data with ggplot\nHW3, RP6\n\n\n3-4 [html], [slides], [pdf]\nManaging data frames with dplyr\nJB5–7, HW5, RP3\n\n\n\nFilter, select, arrange, mutate, summarize\nSome from DSFI\n\n\n5-6 [html], [slides], [pdf]\nEDA checklist\nRP4, HW7\n\n\n\nRight questions, correlation, missing values, outliers\n\n\n\n7-8 [html], [slides], [pdf]\nReshaping, tidying, joining dataframes\nJB8, JB14–16, HW12–13\n\n\n\ntidyr, more dplyr\n\n\n\n9–10\nPrinciples of good graphics\nJB24–27, RP5, RP14–15\n\n\n11–12\nExploring time series data\nJB10–13, HW14–16\n\n\n13\nExploring spatial data\n\n\n\n14\nmapview, leaflet, sf\n\n\n\n15\nFunctional programming with R\n\n\n\n16\nWriting functions, purrr\n\n\n\n17\nVisualising data using animations\nRP11–12\n\n\n18\ngganimate\n\n\n\n19–20\nVersion control with Git and GitHub\nJB3, some from DSFI\n\n\n21\nDashboards\nJB42\n\n\n22\nflexdashboard, shiny\n\n\n\n23–24\nDimensionality reduction and Clustering\nRP13"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "University of Cape Town STA5092Z - Exploratory Data Analysis",
    "section": "",
    "text": "The idea for the course is to be very applied and hands-on, and based around worked examples and case studies, with a focus on the code used to generate an analysis.\nThe course is entirely in R, and the goal of the course is as much to introduce students to R and develop R skills as to cover the theory of EDA.\n\n\n24 lectures in classroom environment Mon/Wed/Fri 4-6pm.\n8 lectures by Drs ebnem Er, 16 lectures by Mr Mzabalazo Ngwenya\n\n\n\n\n\n\nPhoto credit Amanda Torr\n\n\nDr. Sebnem Er (she/her) is a senior lecturer at UCT, Statistical Sciences Department, and program coordinator for MSc Data Science degree at UCT.\nI am located at PD Hahn Building 5th Floor, Room 5.55 however I will not be always in my office. If you would like to see me in person in my office please schedule a meeting by sending an email: Sebnem.Er@uct.ac.za\n\n\n\n|Component|Weight| |Class exercises | 20%| |Assignment1|15%| |Assignment2|45%| |Exam|20%|\n\n\n\nExploratory Data Analysis with R by Roger Peng (RP)\nSTA545: Data wrangling, exploration, and analysis with R by Jenny Bryan (JB))\nR for data science by Hadley Wickham (HW)\nExploratory Data Analysis, 1977 Addison-Wesley Publishing Company (John Wilder Tukey)\nExploratory Data Analysis: Past, Present and Future, 1993 Technical Report (John W Tukey)\nHow to look at data: A review of John W. Tukey’s Exploratory Data Analysis, 1979 (Russell M. Church\nCOURSE MATERIALS\nThere is no prescribed textbook for this course. Please follow the github link provided.\nDULY PERFORMED AND EXAMINATION REQUIREMENTS\n9. NOTICES\nAll correspondence with the department must contain your student number.\n10. COMPUTER USE\nYou will have access to SCILAB C, D for several services: the Web, e-MAIL, R, WORD, and EXCEL.\n\n\n\nAll content for this course is available in this link. You will find all the necessary slides, pdfs, R examples in this link."
  },
  {
    "objectID": "index.html#structure-of-the-course",
    "href": "index.html#structure-of-the-course",
    "title": "University of Cape Town STA5092Z - Exploratory Data Analysis",
    "section": "",
    "text": "24 lectures in classroom environment Mon/Wed/Fri 4-6pm.\n8 lectures by Drs ebnem Er, 16 lectures by Mr Mzabalazo Ngwenya"
  },
  {
    "objectID": "index.html#lecturers",
    "href": "index.html#lecturers",
    "title": "University of Cape Town STA5092Z - Exploratory Data Analysis",
    "section": "",
    "text": "Photo credit Amanda Torr\n\n\nDr. Sebnem Er (she/her) is a senior lecturer at UCT, Statistical Sciences Department, and program coordinator for MSc Data Science degree at UCT.\nI am located at PD Hahn Building 5th Floor, Room 5.55 however I will not be always in my office. If you would like to see me in person in my office please schedule a meeting by sending an email: Sebnem.Er@uct.ac.za"
  },
  {
    "objectID": "index.html#grading",
    "href": "index.html#grading",
    "title": "University of Cape Town STA5092Z - Exploratory Data Analysis",
    "section": "",
    "text": "|Component|Weight| |Class exercises | 20%| |Assignment1|15%| |Assignment2|45%| |Exam|20%|"
  },
  {
    "objectID": "index.html#resources",
    "href": "index.html#resources",
    "title": "University of Cape Town STA5092Z - Exploratory Data Analysis",
    "section": "",
    "text": "Exploratory Data Analysis with R by Roger Peng (RP)\nSTA545: Data wrangling, exploration, and analysis with R by Jenny Bryan (JB))\nR for data science by Hadley Wickham (HW)\nExploratory Data Analysis, 1977 Addison-Wesley Publishing Company (John Wilder Tukey)\nExploratory Data Analysis: Past, Present and Future, 1993 Technical Report (John W Tukey)\nHow to look at data: A review of John W. Tukey’s Exploratory Data Analysis, 1979 (Russell M. Church\nCOURSE MATERIALS\nThere is no prescribed textbook for this course. Please follow the github link provided.\nDULY PERFORMED AND EXAMINATION REQUIREMENTS\n9. NOTICES\nAll correspondence with the department must contain your student number.\n10. COMPUTER USE\nYou will have access to SCILAB C, D for several services: the Web, e-MAIL, R, WORD, and EXCEL."
  },
  {
    "objectID": "index.html#lecture-content",
    "href": "index.html#lecture-content",
    "title": "University of Cape Town STA5092Z - Exploratory Data Analysis",
    "section": "",
    "text": "All content for this course is available in this link. You will find all the necessary slides, pdfs, R examples in this link."
  },
  {
    "objectID": "L1-2/lecture1_2.html",
    "href": "L1-2/lecture1_2.html",
    "title": "Lecture 1-2",
    "section": "",
    "text": "Reading\n\n\n\nWhat is EDA?\nSecondary Analysis of Electronic Health Records Book Chapter 15 - Exploratory Data Analysis\nTukey’s EDA book and terminology\n\n\n\nExploratory Data Analysis (EDA) is one of the key preliminary steps any data scientist who deals with data needs to perform before doing any analysis.\n\nEDA is performed to understand your data, to unearth the underlying structure, to assess the quality of your data by means of summary statistics and visual representations, to discover the main attributes and characteristics of each variable in your data, to discover the relationships between your variables in your data and of course to gain insights before performing any complex modelling.\n\nEDA is not done because it has to be done or because it is one of the requirements of a data science project, it is done for scientific reasoning, not housekeeping and to discover what you don’t yet know you should model. Of course, as in any project, you should start your thinking process around the hypotheses you might already have, the questions you want to answer, however, EDA is an open-ended, iterative process. It needs to be reproducible. Do not save output!\n\nIn this course, we will only use R and teach in R. We assume you know R or learned R in the STA5075Z - Statistical Computing in R course. I need to mention that R is just a tool, and there are several other tools to use such as Python. EDA is not about learning R, or making plots in R etc. — it’s about thinking statistically with data to help modelling and decision-making, regardless of the tool.\nAlso, be forewarned that you need to know basic statistics to be able to follow this course and if you are rusty in your first year statistics knowledge, then please read STA1000 book to refreshen your background. The aim of this course and these notes is to cover some of the more popular methods for exploring multivariate data. The perspective that we will take when looking at these techniques will be to use the minimum amount of mathematics necessary for a solid understanding of the techniques and their interpretation. However, this does not mean “no mathematics”! Over the past twenty or so years, modern statistical software packages have made it possible to run all of the techniques that we’ll cover in this course with a few clicks of a mouse, without knowing a single bit of mathematics and almost nothing about how the techniques themselves work. Clicking a mouse might give you results, but it is very difficult to know whether these results are reliable unless you know something about the underlying technique and what potential pitfalls exist. All statistical techniques, and particularly the multivariate ones, make some assumptions about the type and amount of data that should be collected and the aims of the researcher. If these are ignored, the results may not just be incorrect but misleading. In this case it would be better to put the output of an analysis in a rubbish bin than into a report or on a manager’s desk. To get this understanding, a certain amount of mathematics is needed.\nHaving said that, the focus of the course is on the practical use and interpretation of the techniques in the analysis of real-world examples. The kind of statistics and mathematics that will used includes the following topics that have been covered in previous courses:\nIf you are unfamiliar with any of this material, it is important to go back and revise in the first few weeks of the course."
  },
  {
    "objectID": "L1-2/lecture1_2.html#overview",
    "href": "L1-2/lecture1_2.html#overview",
    "title": "Lecture 1-2",
    "section": "",
    "text": "Reading\n\n\n\nWhat is EDA?\nSecondary Analysis of Electronic Health Records Book Chapter 15 - Exploratory Data Analysis\nTukey’s EDA book and terminology\n\n\n\nExploratory Data Analysis (EDA) is one of the key preliminary steps any data scientist who deals with data needs to perform before doing any analysis.\n\nEDA is performed to understand your data, to unearth the underlying structure, to assess the quality of your data by means of summary statistics and visual representations, to discover the main attributes and characteristics of each variable in your data, to discover the relationships between your variables in your data and of course to gain insights before performing any complex modelling.\n\nEDA is not done because it has to be done or because it is one of the requirements of a data science project, it is done for scientific reasoning, not housekeeping and to discover what you don’t yet know you should model. Of course, as in any project, you should start your thinking process around the hypotheses you might already have, the questions you want to answer, however, EDA is an open-ended, iterative process. It needs to be reproducible. Do not save output!\n\nIn this course, we will only use R and teach in R. We assume you know R or learned R in the STA5075Z - Statistical Computing in R course. I need to mention that R is just a tool, and there are several other tools to use such as Python. EDA is not about learning R, or making plots in R etc. — it’s about thinking statistically with data to help modelling and decision-making, regardless of the tool.\nAlso, be forewarned that you need to know basic statistics to be able to follow this course and if you are rusty in your first year statistics knowledge, then please read STA1000 book to refreshen your background. The aim of this course and these notes is to cover some of the more popular methods for exploring multivariate data. The perspective that we will take when looking at these techniques will be to use the minimum amount of mathematics necessary for a solid understanding of the techniques and their interpretation. However, this does not mean “no mathematics”! Over the past twenty or so years, modern statistical software packages have made it possible to run all of the techniques that we’ll cover in this course with a few clicks of a mouse, without knowing a single bit of mathematics and almost nothing about how the techniques themselves work. Clicking a mouse might give you results, but it is very difficult to know whether these results are reliable unless you know something about the underlying technique and what potential pitfalls exist. All statistical techniques, and particularly the multivariate ones, make some assumptions about the type and amount of data that should be collected and the aims of the researcher. If these are ignored, the results may not just be incorrect but misleading. In this case it would be better to put the output of an analysis in a rubbish bin than into a report or on a manager’s desk. To get this understanding, a certain amount of mathematics is needed.\nHaving said that, the focus of the course is on the practical use and interpretation of the techniques in the analysis of real-world examples. The kind of statistics and mathematics that will used includes the following topics that have been covered in previous courses:\nIf you are unfamiliar with any of this material, it is important to go back and revise in the first few weeks of the course."
  },
  {
    "objectID": "L1-2/lecture1_2.html#outline-of-the-course-notes",
    "href": "L1-2/lecture1_2.html#outline-of-the-course-notes",
    "title": "Lecture 1-2",
    "section": "Outline of the course notes",
    "text": "Outline of the course notes\nApart from this general introduction chapter, the course is divided into two parts of roughly equal size. In the first part, we look at techniques that a primarily ways of summarising large amounts of data and extracting its key meaning. Summarisation is concerned with taking a large amount of data and condensing it into a simpler form that is easier to read and understand. Everyone is familiar with the idea of a “summary” section at the back of a textbook chapter, which gives the key ideas contained in the chapter. You can think of the techniques contained in the first part of the course as a summary section for numbers. We look at three techniques in Part I: correspondence analysis, factor analysis, and cluster analysis. Importantly, the techniques in Part I do not attempt to predict anything, nor explain any dependent variable. In fact, there is no dependent variable for any of the techniques in part one. This is left until Part II, which deals with what we will call “predictive” techniques. These techniques attempt to use one set of variables (the independent or predictor variables) to predict one of more other variables (the dependent or outcome variables). Multiple regression, which you would have covered in previous courses, is a typical example of a predictive technique, which we will briefly look at in this course in order to introduce other predictive models. The other techniques we will look at in Part II are: analysis of variance and covariance, discriminant analysis, classification trees, and structural equation modelling. Thus, over the course of the semester we aim to cover nine techniques which cover the bulk of multivariate analyses done in business- and social-research industries today.\nEach technique is illustrated with a detailed example, with more concise descriptions of further examples given in the final part of each chapter, called “Further examples”. These are intended as a basis for discussion in class or for self-study. For the practical implementation of the methods studied in this course, the R software package will be used. The R system is an open-source software project for analysing data and constructing graphics. It provides a general computer language for performing tasks like organizing data, statistical analyses, simulation studies, model fitting, building of complex graphics and many more. The R language was introduced in 1996, but in the first decade of the twenty-first century interest in R has exceeded all possible expectations. Apart from a well maintained core system with new releases every few months there are currently literally thousands of researchers contributing add-on packages on cutting-edge developments in statistics and data analysis. R is available in the Scilabs on campus. If you would like to install R on your own laptop / PC, go to the website http://www.R-project.org. To download R to your own computer: Navigate to …./bin/windows/base and save the file R-x.0.x.-win.exe on your computer. Click this file to start the installation procedure and select the defaults unless you have a good reason not to do so. The core R system that is installed includes several packages. Apart from these installed packages several thousands of dedicated contributed packages are available to be downloaded by users in need of specific analyses. Many users of R prefer working with RStudio. This is a free and open source integrated development environment for R which works with the standard version of R available from CRAN (Comprehensive R Archive Network available at the website address given above). It can be downloaded from the RStudio home page www.rstudio.com to be run from your desktop (Windows, Mac or Linux). In this course we will be using the RStudio environment.\nBefore diving into the techniques, it is necessary to make sure that everyone is on the same mathematical footing, at least as far as basics are concerned. The remainder of this section revises some of the most important ideas behind matrices and the data filling them, and describes how mathematical notation will be used in the remainder of the notes. It may be useful to have a quick read through these sections now to see how familiar you are with the content, and then to refer back to them as you read through the chapters to come."
  },
  {
    "objectID": "L1-2/lecture1_2.html#data-and-the-data-matrix",
    "href": "L1-2/lecture1_2.html#data-and-the-data-matrix",
    "title": "Lecture 1-2",
    "section": "Data and the data matrix",
    "text": "Data and the data matrix\nIn earlier mathematics courses, you would have learned that a matrix is simply a rectangular or square arrangement of numbers. For example,\n\\[\n\\mathbf{X} = \\begin{bmatrix}\n10 & 3 & 49 & 23 \\\\\n9 & 20 & 94 & 1\n\\end{bmatrix}\n\\]\nis a matrix. Specifically it is a matrix with dimension \\(2\\times 4\\), or a “\\(2\\times 4\\) matrix” for short, because it has two rows and four columns. A vector is a special case of a matrix with only one row (called a row vector) or one column (called a column vector). Of course, any number on its own is also a special case of a matrix; one with one row and one column. This is called a scalar.\nIf we want to talk generally about matrices, without referring to specific number like in the matrix above, it is common to use \\(x\\)’s in place of the numbers in the matrix e.g.\n\\[\n\\mathbf{X} = \\begin{bmatrix}\nx_{11} & x_{12} & x_{13} & x_{14}\\\\\nx_{21} & x_{22} & x_{23} & x_{24}\n\\end{bmatrix}\n\\]\nThe whole matrix is usually denoted with a bold capital i.e. \\(\\mathbf{X}\\). It is important to realise that each \\(x\\) in the matrix above is simply a placeholder for a particular value to come. The first matrix (with the numbers) can only refer to one particular matrix, but the second one (with the \\(x\\)’s) can be used to refer to any \\(2\\times 4\\) matrix. Also, we can refer to any position in the \\(\\mathbf{X}\\) matrix using subscript notation.\nTake \\(x_{23}\\) for example. The “23” is actually two subscripts put together (a “2” and a “3”). The first subscript (the “2”) indicates what row the particular \\(x\\) we are interested in is in (the second row), and the second subscript (the “3”) indicates what column the \\(x\\) is in (the third column). In the first matrix above, \\(x_{23}=94\\). Of course, if we have a vector, then there will be only one subscript, because all elements are in the same row (or column if it is a column vector). In fact, there is nothing stopping us having more than two subscripts too (for matrices of more than two dimensions), but we will not need to go this extra step for this course.\nWhy are we going over all this? Suppose that we have given out a survey and collected responses from 6 people on 5 questions. We can arrange these responses in the format of a table, as shown below:\n\\[\n\\mathbf{X} = \\begin{bmatrix}\nx_{11} & x_{12} & x_{13} & x_{14} & x_{15} \\\\\nx_{21} & x_{22} & x_{23} & x_{24} & x_{25} \\\\\nx_{31} & x_{32} & x_{33} & x_{34} & x_{35} \\\\\nx_{41} & x_{42} & x_{43} & x_{44} & x_{45} \\\\\nx_{51} & x_{52} & x_{53} & x_{54} & x_{55} \\\\\nx_{61} & x_{62} & x_{63} & x_{64} & x_{65}\n\\end{bmatrix}\n\\]\nWe will call a table or matrix that is set up like this to contain data collected from a survey or some other piece of research a data table or a data matrix. There is not real difference between it an the other matrices we were talking about earlier, just a special application. The things that we are collecting data from (which could be people, shares, countries, animal species, songs \\(\\dots\\) anything you can collect data on) are called cases or responses. These appear as separate rows in the data matrix. The pieces of information that we use to describe each case are called variables or attributes and these appear in the columns of the data matrix. The \\(x\\)’s, remember, are simply placeholders for values to come. Specifically, the values to come may be numbers, or they may be words. It is perfectly allowable for the first column of \\(x\\)’s to be, for example, the first names of each person, e.g. \\(x_{11}=\\text{Iris}\\). Of course, this will affect the type of analysis we can do later on that variable (for example, it wouldn’t make sense to calculate a mean’ first name).\nR makes a distinction between matrices of numeric values and data frames containing all different types of data.\n\nload(\"data/survey.data.RData\")\nsurvey.data\n\n  Person Q1     Q2       Q3       Q4 Q5\n1   John  6 639020 52.82732 21.91701 16\n2  Sally  1 153860 47.71114 17.41023 14\n3   Jane  4 138180 51.62570 21.96218 16\n4    Tom  2 101360 48.69179 18.88657 13\n5   Rick  2 564000 48.81529 19.48836 11\n6    Amy  3 328830 49.02871 21.84143  9\n\n\nAbove we have a data frame called survey.data. We can ask R whether survey.data is a data frame with the function is.data.frame():\n\nis.data.frame(survey.data)\n\n[1] TRUE\n\n\nWe can ask R whether survey.data is a matrix with the function is.matrix().\n\nis.matrix(survey.data)\n\n[1] FALSE\n\n\nWe can convert a data frame to a matrix with the function as.matrix(). Similarly, a matrix can be converted into a data frame with the function as.data.frame().\n\nX &lt;- as.matrix(survey.data[,-1])\nX\n\n     Q1     Q2       Q3       Q4 Q5\n[1,]  6 639020 52.82732 21.91701 16\n[2,]  1 153860 47.71114 17.41023 14\n[3,]  4 138180 51.62570 21.96218 16\n[4,]  2 101360 48.69179 18.88657 13\n[5,]  2 564000 48.81529 19.48836 11\n[6,]  3 328830 49.02871 21.84143  9\n\n\nAbove we have the matrix called X.\n\nis.data.frame(X)\n\n[1] FALSE\n\nis.matrix(X)\n\n[1] TRUE\n\n\nNotice that when the matrix X was created from the data frame survey.data, the first column, containing non-numeric values were excluded. Has it not been excluded, all entries in the matrix will be converted to text, even the numeric values.\n\nas.matrix(survey.data)\n\n     Person  Q1  Q2       Q3         Q4         Q5  \n[1,] \"John\"  \"6\" \"639020\" \"52.82732\" \"21.91701\" \"16\"\n[2,] \"Sally\" \"1\" \"153860\" \"47.71114\" \"17.41023\" \"14\"\n[3,] \"Jane\"  \"4\" \"138180\" \"51.62570\" \"21.96218\" \"16\"\n[4,] \"Tom\"   \"2\" \"101360\" \"48.69179\" \"18.88657\" \"13\"\n[5,] \"Rick\"  \"2\" \"564000\" \"48.81529\" \"19.48836\" \"11\"\n[6,] \"Amy\"   \"3\" \"328830\" \"49.02871\" \"21.84143\" \" 9\""
  },
  {
    "objectID": "L1-2/lecture1_2.html#standardisation-of-data",
    "href": "L1-2/lecture1_2.html#standardisation-of-data",
    "title": "Lecture 1-2",
    "section": "Standardisation of data",
    "text": "Standardisation of data\nWhen analysing numerical data, it often happens that different variables are measured on scales of very different sizes. For example, in the above matrix the first question might ask one how many children one has and the second question might ask for one’s income in Rands. Clearly, the scale of possible values for the first question (between 0 and perhaps 15) is much smaller than for the second (between 0 and perhaps several million Rand). For reasons that will become clearer later on, this can cause enormous problems in some multivariate techniques by giving too much influence to the variables measured on larger scales. In order to put all variables on an equal footing, it is often necessary to standardise the data. Because several techniques require standardised data we consider it in this introductory chapter, but it is important to realise that not all the techniques need the data to be standardised. Moreover, in cases where all numerical variables are measured on the same scale (e.g.all on a 1 to 5 Likert rating scale) there will be no need to standardise either.\nThere are several different ways to standardise data, but the only one that we will use is to standardise the data so that each variable has a mean of zero and a standard deviation of one. In order to do this we carry out the following steps:\nWe will illustrate the standardisation of a data matrix using the following example. Suppose that information on three variables (income, number of children, and age) has been collected from five individuals. The data is contained in the following table.\n\n\n\nTable 1: Unstandardized Data with Summary Statistics\n\n\n\n\n\nPerson\nIncome\nNo Children\nAge\n\n\n\n\n\\(a\\)\n10000\n0\n40\n\n\n\\(b\\)\n0\n3\n23\n\n\n\\(c\\)\n300000\n2\n32\n\n\n\\(d\\)\n150000\n2\n35\n\n\n\\(e\\)\n1000000\n1\n58\n\n\n\\(\\bar{x}\\)\n292000\n1.6\n37.6\n\n\n\\(s\\)\n414210\n1.140\n12.973\n\n\n\n\n\n\nwhere we use the usual mathematical notation \\(\\bar{x}\\) to denote the mean and \\(s\\) to denote the standard deviation. Note that the variables are measured on very different scales. To standardise the data, we simply follow the steps above. For example, the standardised income of person \\(a\\) is given by \\[\\frac{10\\,000-292\\,000}{414\\,210}=-0.681\\] to three decimal places. Similarly the standardised number of children for person \\(d\\) is given by \\((2-1.6)/1.140=0.351\\). You can check for yourself that the new column means and standard deviations are all zero and one respectively. Since the mean of all the variables is zero, it is possible to see at a glance which observations are below average (those that are negative) and which are above average (those that are positive).\n\n\n\n\n\n\nThe relevance of standardising data may not seem clear to you at the moment. Just bear this section in mind as you continue through the notes and refer back to it when the issue of standardisation reappears.\n\nX &lt;- matrix (c(10000,0,300000,150000,1000000,0,3,2,2,1,40,23,32,35,58), \n             ncol=3,\n             dimnames=list(c(\"a\",\"b\",\"c\",\"d\",\"e\"), \n                           c(\"Income\",\"No Children\",\"Age\")))\n\nIn R we can create a matrix with the matrix() function. The values in the matrix are concatenated with the operator c(). Notice that the values needs to be entered column wise by default. The names for the two dimensions are specified by dimnames=list(“row names”, “column names”). Notice below that the row names appear to the left.\nThey are text, but are not part of the CONTENT of the matrix. The matrix X:5 × 3 contains only numeric values.\n\nX\n\n   Income No Children Age\na   10000           0  40\nb       0           3  23\nc  300000           2  32\nd  150000           2  35\ne 1000000           1  58\n\n\nTo calculate the means we apply to X, column wise (indicated by 2; 1 for row wise) the function mean().\n\nxbar &lt;- apply(X,2,mean)\nxbar\n\n     Income No Children         Age \n   292000.0         1.6        37.6 \n\n\nSimilarly, the function sd() is applied to each column to calculate the standard deviations.\n\ns &lt;- apply(X,2,sd)\ns\n\n      Income  No Children          Age \n4.142101e+05 1.140175e+00 1.297305e+01 \n\n\nAny numeric calculations can be performed by simply typing the expression at the R command prompt “&gt;”.\n\n(10000-292000)/414210\n\n[1] -0.6808141\n\n\nR has the ability to operate on a whole vector (or matrix) at once. Here the standardised values for Age is calculated by subtracting the mean from the values in column 2 and dividing resulting “column minus mean” by the standard deviation.\n\n(X[,2]-1.6)/1.14\n\n         a          b          c          d          e \n-1.4035088  1.2280702  0.3508772  0.3508772 -0.5263158 \n\n\nThe expressions above is simply for illustration purposes. The function scale() performs all the standardisation calculations in a single step. The output is again a matrix of size 5 × 3, but additional attributes are provided: first the mean called “scaled:center”, then the standard deviations called “scaled:scale”.\n\nscale(X)\n\n       Income No Children        Age\na -0.68081393  -1.4032928  0.1849989\nb -0.70495627   1.2278812 -1.1254101\nc  0.01931387   0.3508232 -0.4316641\nd -0.34282120   0.3508232 -0.2004155\ne  1.70927752  -0.5262348  1.5724908\nattr(,\"scaled:center\")\n     Income No Children         Age \n   292000.0         1.6        37.6 \nattr(,\"scaled:scale\")\n      Income  No Children          Age \n4.142101e+05 1.140175e+00 1.297305e+01 \n\n\nLet us begin by taking another look at the general \\(\\mathbf{X}\\) matrix from earlier in the chapter.\n\\[\n\\mathbf{X} = \\begin{bmatrix}\nx_{11} & x_{12} & x_{13} & x_{14} & x_{15} \\\\\nx_{21} & x_{22} & x_{23} & x_{24} & x_{25} \\\\\nx_{31} & x_{32} & x_{33} & x_{34} & x_{35} \\\\\nx_{41} & x_{42} & x_{43} & x_{44} & x_{45} \\\\\nx_{51} & x_{52} & x_{53} & x_{54} & x_{55} \\\\\nx_{61} & x_{62} & x_{63} & x_{64} & x_{65}\n\\end{bmatrix}\n\\]\nWe have already discussed the use of subscript notation, using the example of \\(x_{23}\\) – where the first subscript that the \\(x\\) we are interested in is in the second row and the third column of the data matrix. We can make this one step more general by referring to a general subscript \\(i\\) for the rows and \\(j\\) for the columns, to give \\(x_{ij}\\). In the same way that the \\(x\\)’s are just placeholders for value to come, so is \\(i\\) and so is \\(j\\). Thus, we can insert any value from 1 to 6 into \\(i\\) and any value from 1 to 5 into \\(j\\), and refer to a specific element of \\(\\mathbf{X}\\).\nThis becomes important because we don’t want to have to write out the whole matrix every time we want to do something with \\(\\mathbf{X}\\). The general \\(i\\) and \\(j\\) subscripts allow us to be much more concise. For example, suppose that the \\(x\\)’s are scores on 5 different tests. The score achieved by student \\(i\\) on test \\(j\\) is given by \\(x_{ij}\\). Suppose now that we want to find student 3’s average mark, which we label as \\(\\bar{x}_3\\). In order to do this we need to add up all 5 test scores and divide by 5. We could write \\[\\bar{x}_3=\\frac{x_{31}+x_{32}+x_{33}+x_{34}+x_{35}}{5}\\] or we could write \\[\\bar{x}_3=\\frac{1}{5}\\sum_{j=1}^5 x_{3j}\\] The summation sign \\((\\Sigma)\\) indicates that we add up all the terms following the sign, by letting \\(j\\) take each of the values in turn between the “limits of summation” (which are 1 and 5 respectively). In this case, it does not save much time or space to use summation notation, but in some cases it does. For example, if we now want refer to the average test score of any of the individuals in our data set, we have two choices: either use the “full” notation and write out all the averages \\[\\begin{align*}\n\\bar{x}_1&=(x_{11}+x_{12}+x_{13}+x_{14}+x_{15})/5 \\\\\n\\bar{x}_2&=(x_{21}+x_{22}+x_{23}+x_{24}+x_{25})/5 \\\\\n\\bar{x}_3&=(x_{31}+x_{32}+x_{33}+x_{34}+x_{35})/5 \\\\\n\\bar{x}_4&=(x_{41}+x_{42}+x_{43}+x_{44}+x_{45})/5 \\\\\n\\bar{x}_5&=(x_{51}+x_{52}+x_{53}+x_{54}+x_{55})/5 \\\\\n\\bar{x}_6&=(x_{61}+x_{62}+x_{63}+x_{64}+x_{65})/5\n\\end{align*}\\] or use the other general subscript \\(i\\) and write the average test score of person \\(i\\) is given by \\[\\bar{x}_i=\\frac{1}{5}\\sum_{j=1}^5 x_{ij}\\hspace{4em}(i=1,\\dots,6)\\] where the \\((i=1,\\dots,6)\\) part indicates that \\(i\\) can take on any value from 1 to 6. The compactness of the summation notation is clear to see. Now, suppose that we want to weight the different test scores differently. This is typically what happens in the calculation of a year mark. Suppose that the \\(x\\)’s are marks are 5 class tests, which are count \\(w_1\\), \\(w_2\\), \\(w_3\\), \\(w_4\\), \\(w_5\\) towards the final mark. Note that we have introduced another variable here, \\(w_j\\), to denote the weight attached to test \\(j\\). Then the year mark obtained by student \\(i\\) is given by\n\\[\\bar{x}_i=\\frac{w_1x_{i1}+w_2x_{i2}+w_3x_{i3}+w_4x_{i4}+w_5x_{i5}}{5}\\hspace{4em}(i=1,\\dots,6)\\] or we could just write \\[\\bar{x}_i=\\frac{1}{5}\\sum_{j=1}^5 w_jx_{ij}\\hspace{4em}(i=1,\\dots,6)\\] Note that the \\(w\\)’s only have a \\(j\\) subscript because they do not differ over students (the weighting is the same for all students) and so do not need an \\(i\\) subscript. Suppose that the weightings did differ over students (say because the weights needed to be adjusted if students miss a test for medical reasons). Then the \\(w\\)’s would be able to differ over students, we would need to include a subscript \\(i\\), and we would have an average mark given by \\[\\bar{x}_i=\\frac{1}{5}\\sum_{j=1}^5 w_{ij}x_{ij}\\hspace{4em}(i=1,\\dots,6)\\] Try writing out the average year mark for student 4 as a practice exercise. Finally, suppose that we want to work out a class average, labelled \\(C\\). For this we need to add together each student’s average mark \\(\\bar{x}_i\\) and then divide by the number of students in the class, 6. This can be written as\n\\[C=\\sum_{i=1}^6 \\bar{x}_i=\\frac{1}{6}\\sum_{i=1}^6 \\frac{1}{5}\\sum_{j=1}^5 w_{ij}x_{ij}=\\frac{1}{30}\\sum_{i=1}^6 \\sum_{j=1}^5 w_{ij}x_{ij}\\hspace{4em}(i=1,\\dots,6)\\] Once again, try writing this out in full as an exercise and to see the usefulness of the summarised notation!"
  },
  {
    "objectID": "L1-2/lecture1_2.html#introduction-to-singular-value-decomposition-svd",
    "href": "L1-2/lecture1_2.html#introduction-to-singular-value-decomposition-svd",
    "title": "Lecture 1-2",
    "section": "Introduction to Singular Value Decomposition (SVD)",
    "text": "Introduction to Singular Value Decomposition (SVD)\nThere is one result from matrix algebra that we will use extensively in the methods discussed in this course. Without going into the detail of the mathematics, the singular value decomposition (SVD) can be viewed as a “black box”. The results of the SVD can be stated in terms of matrices or individual elements of a matrix.\n\nMathematical Definition\nAny matrix \\(\\mathbf{X}\\) can be expressed as the product of three matrices \\(\\mathbf{U}\\), \\(\\mathbf{D}\\), and \\(\\mathbf{V}'\\). The \\(ij\\)-th element of \\(\\mathbf{X}\\), denoted as \\(x_{ij}\\), is expressed as:\n\\[x_{ij} = \\sum_{k=1}^r u_{ik} d_k v_{jk}\\]\nThe values \\(d_k\\) have only one subscript because the matrix \\(\\mathbf{D}\\) is a diagonal matrix with zeros for all off-diagonal elements.\nIf \\(\\mathbf{X}\\) is the matrix of standardised data (e.g., Income, Number of Children, and Age), then \\(r=3\\), and \\(\\mathbf{D}\\) will contain three diagonal values. These values in \\(\\mathbf{D}\\) are called the singular values. Singular values are always non-negative (\\(d_k \\ge 0\\)), and we order them such that \\(d_1 \\ge d_2 \\ge \\dots \\ge d_r\\)."
  },
  {
    "objectID": "L1-2/lecture1_2.html#dimension-reduction",
    "href": "L1-2/lecture1_2.html#dimension-reduction",
    "title": "Lecture 1-2",
    "section": "Dimension Reduction",
    "text": "Dimension Reduction\nThe SVD is the basis for approximating multivariate data by dimension reduction. Working with too many variables makes it difficult to discern interrelationships. We often seek a matrix \\(\\mathbf{X}^*\\) that is “simpler” than \\(\\mathbf{X}\\) but remains a good approximation.\n\nLeast Squares Approximation\nWe aim to find the least squares solution \\(\\mathbf{X}^*\\) that minimizes the sum of squared differences between the elements of \\(\\mathbf{X}\\) and \\(\\mathbf{X}^*\\):\n\\[\\min \\sum_{i} \\sum_{j} (x_{ij} - x_{ij}^*)^2\\]\nAccording to Huygens’ Principle, the approximation necessarily includes the centroid (mean), so we center the data matrix before approximating. If the data is already standardised, it is already centered.\nBest 2D approximation: \\(\\mathbf{X}^* = \\sum_{k=1}^2 u_{ik} d_k v_{jk}\\) Best 1D approximation: \\(\\mathbf{X}^* = u_{i1} d_1 v_{j1}\\)"
  },
  {
    "objectID": "L1-2/lecture1_2.html#r-programming-exercise",
    "href": "L1-2/lecture1_2.html#r-programming-exercise",
    "title": "Lecture 1-2",
    "section": "R Programming Exercise",
    "text": "R Programming Exercise\nUse the interactive console below to perform the SVD on a standardized dataset.\n\n# Step 1: Create the sample data matrix X\nX &lt;- matrix(c(50, 2, 35, \n              20, 4, 45, \n              40, 3, 30, \n              35, 3, 32, \n              60, 5, 25), \n            nrow = 5, byrow = TRUE)\ncolnames(X) &lt;- c(\"Income\", \"Children\", \"Age\")\n\n# Step 2: Standardise the matrix\nX.std &lt;- scale(X)\n\n# Step 3: Compute SVD\nres.svd &lt;- svd(X.std)\n\n# Extract components\nU &lt;- res.svd$u\nD &lt;- diag(res.svd$d)\nV &lt;- res.svd$v\n\n# Step 4: Construct the best 2-dimensional approximation (X.star)\n# We use only the first two columns/elements\nX.star &lt;- U[, 1:2] %*% D[1:2, 1:2] %*% t(V[, 1:2])\n\n# View results\nprint(\"Standardized Matrix:\")\n\n[1] \"Standardized Matrix:\"\n\nprint(X.std)\n\n          Income   Children        Age\n[1,]  0.59344243 -1.2278812  0.2151580\n[2,] -1.38469899  0.5262348  1.5598952\n[3,] -0.06593805 -0.3508232 -0.4572107\n[4,] -0.39562828 -0.3508232 -0.1882632\n[5,]  1.25282290  1.4032928 -1.1295793\nattr(,\"scaled:center\")\n  Income Children      Age \n    41.0      3.4     33.4 \nattr(,\"scaled:scale\")\n   Income  Children       Age \n15.165751  1.140175  7.436397 \n\nprint(\"2D Approximation (X.star):\")\n\n[1] \"2D Approximation (X.star):\"\n\nprint(X.star)\n\n            [,1]       [,2]       [,3]\n[1,]  0.26048350 -1.2630881 -0.1244430\n[2,] -1.51239121  0.5127327  1.4296557\n[3,]  0.21160127 -0.3214764 -0.1741349\n[4,] -0.09109489 -0.3186221  0.1223452\n[5,]  1.13140134  1.3904538 -1.2534230"
  },
  {
    "objectID": "L1-2/lecture1_2.html#a-word-of-caution-on-practical-data-analysis",
    "href": "L1-2/lecture1_2.html#a-word-of-caution-on-practical-data-analysis",
    "title": "Lecture 1-2",
    "section": "A word of caution on practical data analysis",
    "text": "A word of caution on practical data analysis\nOne of the main aims of this course is to put you in a position of being able to perform the multivariate statistical analysis of your own research projects, in whatever field this may be. Most of the examples used in these notes are themselves real-world studies, and so you will get some idea of some of the complexities involved in gathering and analysing data. Having said that, there is an obvious need in an introductory course like this one to choose data sets that work’ and that can be used to illustrate the techniques. We therefore do not discuss many of the practical difficulties which inevitably arise when doing your own original research. As a result when these difficulties arise when it comes to doing your own research, you may look back on this course and think why weren’t we taught that?’ Unfortunately, the kinds of problems that can arise are so varied and require such different solutions that it is not possible to teach in a course such as this one. As Bartholemew et al. put it, only when one has a clear idea of where one is going is it possible to know the important questions which arise”. However, the following broad areas should be borne in mind whenever conducting an original analysis.\n\n\n\n\n\n\nMissing Data\n\n\n\nMissing data can cause severe problems for many of the techniques we will consider. Most techniques will simply drop cases which possess missing data on any of the variables to be included in the analysis. When the number of variables is large, as is often the case in multivariate analyses, this can result in a substantial proportion of the sample being dropped. This proportion should always be noted early in the analysis. Another critical question to ask iswhy is the data missing?” and ’does the missing data introduce any bias into the results?” Often, it is the people with the most extreme views that turn up as missing data by refusing to answer certain questions, which is clearly biasing. Possible solutions are mean replacement or other imputation (replacement) techniques, but these are beyond the scope of this course.\n\n\n\n\n\n\n\n\nSample Sizes\n\n\n\nIt is a general rule that the bigger the model you fit, the greater the number of cases you need. In univariate analysis and simple hypothesis testing, the calculation ofrequired’\nsample sizes is reasonably straightforward, but in multivariate analysis there are only very rough guidelines where any exist at all. As a very rough guideline, most techniques require at least 10 respondents per parameter estimated. That means that in order to estimate a regression model with four independent variable, you need at least 50 respondents (not forgetting the constant term \\(\\beta_0\\), there are 5 parameters to be estimated). When sample sizes are small, one should be very careful about drawing strong conclusions. This is a particular problem in student research, where sample sizes are typically very small."
  },
  {
    "objectID": "githubpracticals/sta_3022_f_prac_0_quarto_worksheet.html",
    "href": "githubpracticals/sta_3022_f_prac_0_quarto_worksheet.html",
    "title": "STA3022F – Computer Practical 0",
    "section": "",
    "text": "By the end of this practical, you should be able to:\n\nNavigate the RStudio environment confidently\nCreate, inspect, and manipulate R objects\nProduce basic graphical summaries\nInterpret univariate and bivariate summaries in preparation for multivariate analysis\nExplain why each tool is useful, not just how to run it"
  },
  {
    "objectID": "githubpracticals/sta_3022_f_prac_0_quarto_worksheet.html#learning-outcomes",
    "href": "githubpracticals/sta_3022_f_prac_0_quarto_worksheet.html#learning-outcomes",
    "title": "STA3022F – Computer Practical 0",
    "section": "",
    "text": "By the end of this practical, you should be able to:\n\nNavigate the RStudio environment confidently\nCreate, inspect, and manipulate R objects\nProduce basic graphical summaries\nInterpret univariate and bivariate summaries in preparation for multivariate analysis\nExplain why each tool is useful, not just how to run it"
  },
  {
    "objectID": "githubpracticals/sta_3022_f_prac_0_quarto_worksheet.html#getting-started-with-r-and-rstudio",
    "href": "githubpracticals/sta_3022_f_prac_0_quarto_worksheet.html#getting-started-with-r-and-rstudio",
    "title": "STA3022F – Computer Practical 0",
    "section": "2 1. Getting started with R and RStudio",
    "text": "2 1. Getting started with R and RStudio\nYou may either:\n\nInstall R and RStudio locally or\n\nUse RStudio Cloud\n\n\nNote: Cloud projects have monthly usage limits. Save and close projects when not in use.\n\n\n2.1 ✏️ Question 1 (conceptual)\n\nWhy is using a scripted statistical environment (like RStudio) preferable to point-and-click software for reproducible data analysis?"
  },
  {
    "objectID": "githubpracticals/sta_3022_f_prac_0_quarto_worksheet.html#the-r-console-first-interactions",
    "href": "githubpracticals/sta_3022_f_prac_0_quarto_worksheet.html#the-r-console-first-interactions",
    "title": "STA3022F – Computer Practical 0",
    "section": "3 2. The R Console: first interactions",
    "text": "3 2. The R Console: first interactions\nRun the following directly in the Console:\n5 - 8\nNow try:\n5 -\nYou should see the + prompt.\n\n3.1 ✏️ Question 2\n\nWhat does the + prompt indicate? How can you return to the normal &gt; prompt?"
  },
  {
    "objectID": "githubpracticals/sta_3022_f_prac_0_quarto_worksheet.html#creating-and-inspecting-objects",
    "href": "githubpracticals/sta_3022_f_prac_0_quarto_worksheet.html#creating-and-inspecting-objects",
    "title": "STA3022F – Computer Practical 0",
    "section": "4 3. Creating and inspecting objects",
    "text": "4 3. Creating and inspecting objects\nxx &lt;- 1:10\nyy &lt;- rnorm(n = 20, mean = 50, sd = 15)\nDisplay each object by typing its name:\nxx\nyy\n\n4.1 ✏️ Question 3\n\nWhat type of object is xx? What distribution was used to generate yy?"
  },
  {
    "objectID": "githubpracticals/sta_3022_f_prac_0_quarto_worksheet.html#functions-and-arguments",
    "href": "githubpracticals/sta_3022_f_prac_0_quarto_worksheet.html#functions-and-arguments",
    "title": "STA3022F – Computer Practical 0",
    "section": "5 4. Functions and arguments",
    "text": "5 4. Functions and arguments\nFunctions in R are also objects. To use a function, supply arguments in parentheses:\nmean(yy)\nsd(yy)\n\n5.1 ✏️ Question 4\n\nExplain the difference between an R object and an R function."
  },
  {
    "objectID": "githubpracticals/sta_3022_f_prac_0_quarto_worksheet.html#reproducibility-and-randomness",
    "href": "githubpracticals/sta_3022_f_prac_0_quarto_worksheet.html#reproducibility-and-randomness",
    "title": "STA3022F – Computer Practical 0",
    "section": "6 5. Reproducibility and randomness",
    "text": "6 5. Reproducibility and randomness\nTo make results reproducible:\nset.seed(3022)\nyy &lt;- rnorm(20, 50, 15)\n\n6.1 ✏️ Question 5\n\nWhy is setting a seed important in simulation-based statistics and multivariate analysis?"
  },
  {
    "objectID": "githubpracticals/sta_3022_f_prac_0_quarto_worksheet.html#the-workspace",
    "href": "githubpracticals/sta_3022_f_prac_0_quarto_worksheet.html#the-workspace",
    "title": "STA3022F – Computer Practical 0",
    "section": "7 6. The Workspace",
    "text": "7 6. The Workspace\nList objects in memory:\nls()\nobjects()\nRemove an object:\nrm(xx)\n\n7.1 ✏️ Question 6\n\nWhy is it good practice to keep your workspace clean when working on large analyses?"
  },
  {
    "objectID": "githubpracticals/sta_3022_f_prac_0_quarto_worksheet.html#basic-graphics",
    "href": "githubpracticals/sta_3022_f_prac_0_quarto_worksheet.html#basic-graphics",
    "title": "STA3022F – Computer Practical 0",
    "section": "8 7. Basic graphics",
    "text": "8 7. Basic graphics\ngr.data &lt;- rnorm(1000)\nhist(gr.data)\n\n8.1 ✏️ Question 7\n\nWhat features of the distribution can you identify from the histogram?"
  },
  {
    "objectID": "githubpracticals/sta_3022_f_prac_0_quarto_worksheet.html#working-with-built-in-datasets",
    "href": "githubpracticals/sta_3022_f_prac_0_quarto_worksheet.html#working-with-built-in-datasets",
    "title": "STA3022F – Computer Practical 0",
    "section": "9 8. Working with built-in datasets",
    "text": "9 8. Working with built-in datasets\ndata()\n?iris\nInspect the dataset:\nstr(iris)\nsummary(iris)\n\n9.1 ✏️ Question 8\n\nWhy is iris considered a multivariate dataset?"
  },
  {
    "objectID": "githubpracticals/sta_3022_f_prac_0_quarto_worksheet.html#boxplots-and-group-comparisons",
    "href": "githubpracticals/sta_3022_f_prac_0_quarto_worksheet.html#boxplots-and-group-comparisons",
    "title": "STA3022F – Computer Practical 0",
    "section": "10 9. Boxplots and group comparisons",
    "text": "10 9. Boxplots and group comparisons\nboxplot(Sepal.Length ~ Species, data = iris)\n\n10.1 ✏️ Question 9\n\nWhich species shows the largest variability in sepal length? How can you tell?\n\n\nThink ahead: Why might unequal variability matter for PCA or clustering?"
  },
  {
    "objectID": "githubpracticals/sta_3022_f_prac_0_quarto_worksheet.html#relationships-between-variables",
    "href": "githubpracticals/sta_3022_f_prac_0_quarto_worksheet.html#relationships-between-variables",
    "title": "STA3022F – Computer Practical 0",
    "section": "11 10. Relationships between variables",
    "text": "11 10. Relationships between variables\nplot(iris$Sepal.Length, iris$Petal.Length)\ncor(iris$Sepal.Length, iris$Petal.Length)\ncor(iris[, 1:4])\n\n11.1 ✏️ Question 10\n\nWhy is a correlation matrix a key input for PCA and factor analysis?"
  },
  {
    "objectID": "githubpracticals/sta_3022_f_prac_0_quarto_worksheet.html#statistical-testing-conceptual-focus",
    "href": "githubpracticals/sta_3022_f_prac_0_quarto_worksheet.html#statistical-testing-conceptual-focus",
    "title": "STA3022F – Computer Practical 0",
    "section": "12 11. Statistical testing (conceptual focus)",
    "text": "12 11. Statistical testing (conceptual focus)\nWe illustrate hypothesis testing using the beaver1 dataset:\n?beaver1\nt.test(beaver1$temp, alternative = \"two.sided\")\n\n12.1 ✏️ Question 11\n\nWhat is the null hypothesis being tested here?"
  },
  {
    "objectID": "githubpracticals/sta_3022_f_prac_0_quarto_worksheet.html#reflection-very-important",
    "href": "githubpracticals/sta_3022_f_prac_0_quarto_worksheet.html#reflection-very-important",
    "title": "STA3022F – Computer Practical 0",
    "section": "13 12. Reflection (very important)",
    "text": "13 12. Reflection (very important)\nWrite 2–3 sentences answering:\n\nWhich graphical summary did you find most informative today?\nWhich dataset felt most multivariate, and why?\nWhat questions would you ask next if more variables were available?"
  },
  {
    "objectID": "githubpracticals/sta_3022_f_prac_0_quarto_worksheet.html#looking-ahead",
    "href": "githubpracticals/sta_3022_f_prac_0_quarto_worksheet.html#looking-ahead",
    "title": "STA3022F – Computer Practical 0",
    "section": "14 Looking ahead",
    "text": "14 Looking ahead\nEverything you did today forms the foundation for:\n\nPrincipal Component Analysis (PCA)\nClustering\nMultivariate regression\nDimension reduction\n\nKeep this worksheet — you will refer back to it often."
  },
  {
    "objectID": "githubpracticals/prac01_shiny/index.html",
    "href": "githubpracticals/prac01_shiny/index.html",
    "title": "Practical 1: PCA",
    "section": "",
    "text": "Edit the code below and click “Run”.\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 700\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  titlePanel(\"Multivariate Stats: Student-Led PCA\"),\n  sidebarLayout(\n    sidebarPanel(\n      # 1. File Upload\n      fileInput(\"file1\", \"Upload CSV File\", accept = \".csv\"),\n      tags$hr(),\n      \n      # 2. Dynamic UI for Variable Selection (appears after upload)\n      uiOutput(\"var_selector\"),\n      \n      # 3. PCA Settings\n      checkboxInput(\"scale\", \"Scale Variables? (Recommended)\", TRUE),\n      tags$hr(),\n      \n      # 4. Axis Selection\n      numericInput(\"pc_x\", \"PC for X-axis:\", 1, min = 1),\n      numericInput(\"pc_y\", \"PC for Y-axis:\", 2, min = 1)\n    ),\n    \n    mainPanel(\n      tabsetPanel(\n        tabPanel(\"Biplot\", plotOutput(\"pca_plot\", height = \"600px\")),\n        tabPanel(\"Summary & Loadings\", verbatimTextOutput(\"pca_summary\")),\n        tabPanel(\"Data Preview\", tableOutput(\"data_preview\"))\n      )\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n  \n  # Reactive: Read data, remove NAs, and keep only numeric\n  raw_data &lt;- reactive({\n    req(input$file1)\n    df &lt;- read.csv(input$file1$datapath)\n    df &lt;- na.omit(df) # Remove rows with NAs\n    # Keep only numeric columns for selection\n    df_num &lt;- df[sapply(df, is.numeric)]\n    return(df_num)\n  })\n\n  # Generate the checklist of variables based on the uploaded file\n  output$var_selector &lt;- renderUI({\n    df &lt;- raw_data()\n    choices &lt;- names(df)\n    checkboxGroupInput(\"selected_vars\", \"Choose Variables for PCA:\", \n                       choices = choices, selected = choices[1:min(3, length(choices))])\n  })\n\n  # Reactive: Perform the PCA\n  pca_res &lt;- reactive({\n    req(input$selected_vars)\n    req(length(input$selected_vars) &gt;= 2) # Need at least 2 vars for PCA\n    \n    data_for_pca &lt;- raw_data()[, input$selected_vars]\n    prcomp(data_for_pca, scale. = input$scale)\n  })\n\n  # Output: Biplot\n  output$pca_plot &lt;- renderPlot({\n    req(pca_res())\n    biplot(pca_res(), choices = c(input$pc_x, input$pc_y), \n           main = \"Interactive PCA Biplot\", cex = 0.8)\n    abline(h = 0, v = 0, lty = 2, col = \"gray\")\n  })\n\n  # Output: Summary\n  output$pca_summary &lt;- renderPrint({\n    req(pca_res())\n    cat(\"Standard Deviations (Eigenvalues):\\n\")\n    print(pca_res()$sdev)\n    cat(\"\\nRotation (Loadings):\\n\")\n    print(pca_res()$rotation)\n    cat(\"\\nSummary of Variance:\\n\")\n    summary(pca_res())\n  })\n\n  # Output: Data Preview\n  output$data_preview &lt;- renderTable({\n    head(raw_data(), 10)\n  })\n}\n\nshinyApp(ui, server)"
  },
  {
    "objectID": "githubpracticals/prac01_shiny/index.html#pca-exercise",
    "href": "githubpracticals/prac01_shiny/index.html#pca-exercise",
    "title": "Practical 1: PCA",
    "section": "",
    "text": "Edit the code below and click “Run”.\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 700\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  titlePanel(\"Multivariate Stats: Student-Led PCA\"),\n  sidebarLayout(\n    sidebarPanel(\n      # 1. File Upload\n      fileInput(\"file1\", \"Upload CSV File\", accept = \".csv\"),\n      tags$hr(),\n      \n      # 2. Dynamic UI for Variable Selection (appears after upload)\n      uiOutput(\"var_selector\"),\n      \n      # 3. PCA Settings\n      checkboxInput(\"scale\", \"Scale Variables? (Recommended)\", TRUE),\n      tags$hr(),\n      \n      # 4. Axis Selection\n      numericInput(\"pc_x\", \"PC for X-axis:\", 1, min = 1),\n      numericInput(\"pc_y\", \"PC for Y-axis:\", 2, min = 1)\n    ),\n    \n    mainPanel(\n      tabsetPanel(\n        tabPanel(\"Biplot\", plotOutput(\"pca_plot\", height = \"600px\")),\n        tabPanel(\"Summary & Loadings\", verbatimTextOutput(\"pca_summary\")),\n        tabPanel(\"Data Preview\", tableOutput(\"data_preview\"))\n      )\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n  \n  # Reactive: Read data, remove NAs, and keep only numeric\n  raw_data &lt;- reactive({\n    req(input$file1)\n    df &lt;- read.csv(input$file1$datapath)\n    df &lt;- na.omit(df) # Remove rows with NAs\n    # Keep only numeric columns for selection\n    df_num &lt;- df[sapply(df, is.numeric)]\n    return(df_num)\n  })\n\n  # Generate the checklist of variables based on the uploaded file\n  output$var_selector &lt;- renderUI({\n    df &lt;- raw_data()\n    choices &lt;- names(df)\n    checkboxGroupInput(\"selected_vars\", \"Choose Variables for PCA:\", \n                       choices = choices, selected = choices[1:min(3, length(choices))])\n  })\n\n  # Reactive: Perform the PCA\n  pca_res &lt;- reactive({\n    req(input$selected_vars)\n    req(length(input$selected_vars) &gt;= 2) # Need at least 2 vars for PCA\n    \n    data_for_pca &lt;- raw_data()[, input$selected_vars]\n    prcomp(data_for_pca, scale. = input$scale)\n  })\n\n  # Output: Biplot\n  output$pca_plot &lt;- renderPlot({\n    req(pca_res())\n    biplot(pca_res(), choices = c(input$pc_x, input$pc_y), \n           main = \"Interactive PCA Biplot\", cex = 0.8)\n    abline(h = 0, v = 0, lty = 2, col = \"gray\")\n  })\n\n  # Output: Summary\n  output$pca_summary &lt;- renderPrint({\n    req(pca_res())\n    cat(\"Standard Deviations (Eigenvalues):\\n\")\n    print(pca_res()$sdev)\n    cat(\"\\nRotation (Loadings):\\n\")\n    print(pca_res()$rotation)\n    cat(\"\\nSummary of Variance:\\n\")\n    summary(pca_res())\n  })\n\n  # Output: Data Preview\n  output$data_preview &lt;- renderTable({\n    head(raw_data(), 10)\n  })\n}\n\nshinyApp(ui, server)"
  },
  {
    "objectID": "githubpracticals/index.html",
    "href": "githubpracticals/index.html",
    "title": "Practical 1: PCA",
    "section": "",
    "text": "Practical 1\n\n\nPractical 2"
  },
  {
    "objectID": "L5-6/lecture5_6.html",
    "href": "L5-6/lecture5_6.html",
    "title": "",
    "section": "",
    "text": "Wait for Fri 20th"
  },
  {
    "objectID": "L3-4/lecture3_4.html",
    "href": "L3-4/lecture3_4.html",
    "title": "",
    "section": "",
    "text": "Wait for Wed 18th"
  },
  {
    "objectID": "L7-8/lecture7_8.html",
    "href": "L7-8/lecture7_8.html",
    "title": "",
    "section": "",
    "text": "Wait for Mon 23rd"
  },
  {
    "objectID": "githubpracticals/prac01/index.html",
    "href": "githubpracticals/prac01/index.html",
    "title": "Practical 1: PCA",
    "section": "",
    "text": "In this section, we will load a dataset. You can use the built-in mtcars or upload your own CSV using the file helper below.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page."
  },
  {
    "objectID": "githubpracticals/prac01/index.html#load-your-data",
    "href": "githubpracticals/prac01/index.html#load-your-data",
    "title": "Practical 1: PCA",
    "section": "",
    "text": "In this section, we will load a dataset. You can use the built-in mtcars or upload your own CSV using the file helper below.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page."
  },
  {
    "objectID": "githubpracticals/prac01/index.html#perform-the-pca",
    "href": "githubpracticals/prac01/index.html#perform-the-pca",
    "title": "Practical 1: PCA",
    "section": "2. Perform the PCA",
    "text": "2. Perform the PCA\nTask: Write the code to perform PCA on the mtcars dataset.\nSelect only columns 1 through 7. Set scale = TRUE. Store the result in an object called my_pca.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page."
  },
  {
    "objectID": "githubpracticals/prac01/index.html#visualization-and-interpretation",
    "href": "githubpracticals/prac01/index.html#visualization-and-interpretation",
    "title": "Practical 1: PCA",
    "section": "3. Visualization and Interpretation",
    "text": "3. Visualization and Interpretation\nNow, use the biplot() function to visualize your results. Try changing the choices argument to look at PC1 vs PC3.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nMini-Quiz\nQuestion: If you change scale = TRUE to scale = FALSE, which variable dominates the first Principal Component? Hint: Look at the variable units/variances."
  },
  {
    "objectID": "githubpracticals/prac2.html",
    "href": "githubpracticals/prac2.html",
    "title": "Prac2",
    "section": "",
    "text": "In this prac\n\n\nRun, then click “Run Code”\n\nPlease enable JavaScript to experience the dynamic code cell content on this page."
  },
  {
    "objectID": "githubpracticals/prac2.html#intro-to-pca",
    "href": "githubpracticals/prac2.html#intro-to-pca",
    "title": "Prac2",
    "section": "",
    "text": "In this prac\n\n\nRun, then click “Run Code”\n\nPlease enable JavaScript to experience the dynamic code cell content on this page."
  },
  {
    "objectID": "githubpracticals/practical0/index.html",
    "href": "githubpracticals/practical0/index.html",
    "title": "STA3022F – Computer Practical 0",
    "section": "",
    "text": "Quarto enables you to weave together content and executable code into a finished presentation. To learn more about Quarto presentations see https://quarto.org/docs/presentations/."
  },
  {
    "objectID": "githubpracticals/practical0/index.html#quarto",
    "href": "githubpracticals/practical0/index.html#quarto",
    "title": "STA3022F – Computer Practical 0",
    "section": "",
    "text": "Quarto enables you to weave together content and executable code into a finished presentation. To learn more about Quarto presentations see https://quarto.org/docs/presentations/."
  },
  {
    "objectID": "githubpracticals/practical0/index.html#bullets",
    "href": "githubpracticals/practical0/index.html#bullets",
    "title": "STA3022F – Computer Practical 0",
    "section": "Bullets",
    "text": "Bullets\nWhen you click the Render button a document will be generated that includes:\n\nContent authored with markdown\nOutput from executable code"
  },
  {
    "objectID": "githubpracticals/practical0/index.html#code",
    "href": "githubpracticals/practical0/index.html#code",
    "title": "STA3022F – Computer Practical 0",
    "section": "Code",
    "text": "Code\nWhen you click the Render button a presentation will be generated that includes both content and the output of embedded code. You can embed code like this:\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "index.slides.html#structure-of-the-course",
    "href": "index.slides.html#structure-of-the-course",
    "title": "University of Cape Town STA5092Z - Exploratory Data Analysis",
    "section": "Structure of the Course",
    "text": "Structure of the Course\n24 lectures in classroom environment Mon/Wed/Fri 4-6pm.\n8 lectures by Drs ebnem Er, 16 lectures by Mr Mzabalazo Ngwenya"
  },
  {
    "objectID": "index.slides.html#lecturers",
    "href": "index.slides.html#lecturers",
    "title": "University of Cape Town STA5092Z - Exploratory Data Analysis",
    "section": "Lecturers",
    "text": "Lecturers\n\nPhoto credit Amanda TorrDr. Sebnem Er (she/her) is a senior lecturer at UCT, Statistical Sciences Department, and program coordinator for MSc Data Science degree at UCT.\nI am located at PD Hahn Building 5th Floor, Room 5.55 however I will not be always in my office. If you would like to see me in person in my office please schedule a meeting by sending an email: Sebnem.Er@uct.ac.za"
  },
  {
    "objectID": "index.slides.html#grading",
    "href": "index.slides.html#grading",
    "title": "University of Cape Town STA5092Z - Exploratory Data Analysis",
    "section": "Grading",
    "text": "Grading\n|Component|Weight| |Class exercises | 20%| |Assignment1|15%| |Assignment2|45%| |Exam|20%|"
  },
  {
    "objectID": "index.slides.html#resources",
    "href": "index.slides.html#resources",
    "title": "University of Cape Town STA5092Z - Exploratory Data Analysis",
    "section": "Resources",
    "text": "Resources\nExploratory Data Analysis with R by Roger Peng (RP)\nSTA545: Data wrangling, exploration, and analysis with R by Jenny Bryan (JB))\nR for data science by Hadley Wickham (HW)\nExploratory Data Analysis, 1977 Addison-Wesley Publishing Company (John Wilder Tukey)\nExploratory Data Analysis: Past, Present and Future, 1993 Technical Report (John W Tukey)\nHow to look at data: A review of John W. Tukey’s Exploratory Data Analysis, 1979 (Russell M. Church\nCOURSE MATERIALS\nThere is no prescribed textbook for this course. Please follow the github link provided.\nDULY PERFORMED AND EXAMINATION REQUIREMENTS\n9. NOTICES\nAll correspondence with the department must contain your student number.\n10. COMPUTER USE\nYou will have access to SCILAB C, D for several services: the Web, e-MAIL, R, WORD, and EXCEL."
  },
  {
    "objectID": "index.slides.html#lecture-content",
    "href": "index.slides.html#lecture-content",
    "title": "University of Cape Town STA5092Z - Exploratory Data Analysis",
    "section": "Lecture Content",
    "text": "Lecture Content\nAll content for this course is available in this link. You will find all the necessary slides, pdfs, R examples in this link."
  },
  {
    "objectID": "L1-2/lecture1_2.slides.html#overview",
    "href": "L1-2/lecture1_2.slides.html#overview",
    "title": "Lecture 1-2",
    "section": "Overview",
    "text": "Overview\n\n\n\n\n\n\n\nReading\n\n\nWhat is EDA?\nSecondary Analysis of Electronic Health Records Book Chapter 15 - Exploratory Data Analysis\nTukey’s EDA book and terminology"
  },
  {
    "objectID": "L1-2/lecture1_2.slides.html#outline-of-the-course-notes",
    "href": "L1-2/lecture1_2.slides.html#outline-of-the-course-notes",
    "title": "Lecture 1-2",
    "section": "Outline of the course notes",
    "text": "Outline of the course notes\nApart from this general introduction chapter, the course is divided into two parts of roughly equal size. In the first part, we look at techniques that a primarily ways of summarising large amounts of data and extracting its key meaning. Summarisation is concerned with taking a large amount of data and condensing it into a simpler form that is easier to read and understand. Everyone is familiar with the idea of a “summary” section at the back of a textbook chapter, which gives the key ideas contained in the chapter. You can think of the techniques contained in the first part of the course as a summary section for numbers. We look at three techniques in Part I: correspondence analysis, factor analysis, and cluster analysis. Importantly, the techniques in Part I do not attempt to predict anything, nor explain any dependent variable. In fact, there is no dependent variable for any of the techniques in part one. This is left until Part II, which deals with what we will call “predictive” techniques. These techniques attempt to use one set of variables (the independent or predictor variables) to predict one of more other variables (the dependent or outcome variables). Multiple regression, which you would have covered in previous courses, is a typical example of a predictive technique, which we will briefly look at in this course in order to introduce other predictive models. The other techniques we will look at in Part II are: analysis of variance and covariance, discriminant analysis, classification trees, and structural equation modelling. Thus, over the course of the semester we aim to cover nine techniques which cover the bulk of multivariate analyses done in business- and social-research industries today.\nEach technique is illustrated with a detailed example, with more concise descriptions of further examples given in the final part of each chapter, called “Further examples”. These are intended as a basis for discussion in class or for self-study. For the practical implementation of the methods studied in this course, the R software package will be used. The R system is an open-source software project for analysing data and constructing graphics. It provides a general computer language for performing tasks like organizing data, statistical analyses, simulation studies, model fitting, building of complex graphics and many more. The R language was introduced in 1996, but in the first decade of the twenty-first century interest in R has exceeded all possible expectations. Apart from a well maintained core system with new releases every few months there are currently literally thousands of researchers contributing add-on packages on cutting-edge developments in statistics and data analysis. R is available in the Scilabs on campus. If you would like to install R on your own laptop / PC, go to the website http://www.R-project.org. To download R to your own computer: Navigate to …./bin/windows/base and save the file R-x.0.x.-win.exe on your computer. Click this file to start the installation procedure and select the defaults unless you have a good reason not to do so. The core R system that is installed includes several packages. Apart from these installed packages several thousands of dedicated contributed packages are available to be downloaded by users in need of specific analyses. Many users of R prefer working with RStudio. This is a free and open source integrated development environment for R which works with the standard version of R available from CRAN (Comprehensive R Archive Network available at the website address given above). It can be downloaded from the RStudio home page www.rstudio.com to be run from your desktop (Windows, Mac or Linux). In this course we will be using the RStudio environment.\nBefore diving into the techniques, it is necessary to make sure that everyone is on the same mathematical footing, at least as far as basics are concerned. The remainder of this section revises some of the most important ideas behind matrices and the data filling them, and describes how mathematical notation will be used in the remainder of the notes. It may be useful to have a quick read through these sections now to see how familiar you are with the content, and then to refer back to them as you read through the chapters to come."
  },
  {
    "objectID": "L1-2/lecture1_2.slides.html#data-and-the-data-matrix",
    "href": "L1-2/lecture1_2.slides.html#data-and-the-data-matrix",
    "title": "Lecture 1-2",
    "section": "Data and the data matrix",
    "text": "Data and the data matrix\nIn earlier mathematics courses, you would have learned that a matrix is simply a rectangular or square arrangement of numbers. For example,\n\\[\n\\mathbf{X} = \\begin{bmatrix}\n10 & 3 & 49 & 23 \\\\\n9 & 20 & 94 & 1\n\\end{bmatrix}\n\\]\nis a matrix. Specifically it is a matrix with dimension \\(2\\times 4\\), or a “\\(2\\times 4\\) matrix” for short, because it has two rows and four columns. A vector is a special case of a matrix with only one row (called a row vector) or one column (called a column vector). Of course, any number on its own is also a special case of a matrix; one with one row and one column. This is called a scalar.\nIf we want to talk generally about matrices, without referring to specific number like in the matrix above, it is common to use \\(x\\)’s in place of the numbers in the matrix e.g.\n\\[\n\\mathbf{X} = \\begin{bmatrix}\nx_{11} & x_{12} & x_{13} & x_{14}\\\\\nx_{21} & x_{22} & x_{23} & x_{24}\n\\end{bmatrix}\n\\]\nThe whole matrix is usually denoted with a bold capital i.e. \\(\\mathbf{X}\\). It is important to realise that each \\(x\\) in the matrix above is simply a placeholder for a particular value to come. The first matrix (with the numbers) can only refer to one particular matrix, but the second one (with the \\(x\\)’s) can be used to refer to any \\(2\\times 4\\) matrix. Also, we can refer to any position in the \\(\\mathbf{X}\\) matrix using subscript notation.\nTake \\(x_{23}\\) for example. The “23” is actually two subscripts put together (a “2” and a “3”). The first subscript (the “2”) indicates what row the particular \\(x\\) we are interested in is in (the second row), and the second subscript (the “3”) indicates what column the \\(x\\) is in (the third column). In the first matrix above, \\(x_{23}=94\\). Of course, if we have a vector, then there will be only one subscript, because all elements are in the same row (or column if it is a column vector). In fact, there is nothing stopping us having more than two subscripts too (for matrices of more than two dimensions), but we will not need to go this extra step for this course.\nWhy are we going over all this? Suppose that we have given out a survey and collected responses from 6 people on 5 questions. We can arrange these responses in the format of a table, as shown below:\n\\[\n\\mathbf{X} = \\begin{bmatrix}\nx_{11} & x_{12} & x_{13} & x_{14} & x_{15} \\\\\nx_{21} & x_{22} & x_{23} & x_{24} & x_{25} \\\\\nx_{31} & x_{32} & x_{33} & x_{34} & x_{35} \\\\\nx_{41} & x_{42} & x_{43} & x_{44} & x_{45} \\\\\nx_{51} & x_{52} & x_{53} & x_{54} & x_{55} \\\\\nx_{61} & x_{62} & x_{63} & x_{64} & x_{65}\n\\end{bmatrix}\n\\]\nWe will call a table or matrix that is set up like this to contain data collected from a survey or some other piece of research a data table or a data matrix. There is not real difference between it an the other matrices we were talking about earlier, just a special application. The things that we are collecting data from (which could be people, shares, countries, animal species, songs \\(\\dots\\) anything you can collect data on) are called cases or responses. These appear as separate rows in the data matrix. The pieces of information that we use to describe each case are called variables or attributes and these appear in the columns of the data matrix. The \\(x\\)’s, remember, are simply placeholders for values to come. Specifically, the values to come may be numbers, or they may be words. It is perfectly allowable for the first column of \\(x\\)’s to be, for example, the first names of each person, e.g. \\(x_{11}=\\text{Iris}\\). Of course, this will affect the type of analysis we can do later on that variable (for example, it wouldn’t make sense to calculate a mean’ first name).\nR makes a distinction between matrices of numeric values and data frames containing all different types of data.\n\nload(\"data/survey.data.RData\")\nsurvey.data\n\n  Person Q1     Q2       Q3       Q4 Q5\n1   John  6 639020 52.82732 21.91701 16\n2  Sally  1 153860 47.71114 17.41023 14\n3   Jane  4 138180 51.62570 21.96218 16\n4    Tom  2 101360 48.69179 18.88657 13\n5   Rick  2 564000 48.81529 19.48836 11\n6    Amy  3 328830 49.02871 21.84143  9\n\n\nAbove we have a data frame called survey.data. We can ask R whether survey.data is a data frame with the function is.data.frame():\n\nis.data.frame(survey.data)\n\n[1] TRUE\n\n\nWe can ask R whether survey.data is a matrix with the function is.matrix().\n\nis.matrix(survey.data)\n\n[1] FALSE\n\n\nWe can convert a data frame to a matrix with the function as.matrix(). Similarly, a matrix can be converted into a data frame with the function as.data.frame().\n\nX &lt;- as.matrix(survey.data[,-1])\nX\n\n     Q1     Q2       Q3       Q4 Q5\n[1,]  6 639020 52.82732 21.91701 16\n[2,]  1 153860 47.71114 17.41023 14\n[3,]  4 138180 51.62570 21.96218 16\n[4,]  2 101360 48.69179 18.88657 13\n[5,]  2 564000 48.81529 19.48836 11\n[6,]  3 328830 49.02871 21.84143  9\n\n\nAbove we have the matrix called X.\n\nis.data.frame(X)\n\n[1] FALSE\n\nis.matrix(X)\n\n[1] TRUE\n\n\nNotice that when the matrix X was created from the data frame survey.data, the first column, containing non-numeric values were excluded. Has it not been excluded, all entries in the matrix will be converted to text, even the numeric values.\n\nas.matrix(survey.data)\n\n     Person  Q1  Q2       Q3         Q4         Q5  \n[1,] \"John\"  \"6\" \"639020\" \"52.82732\" \"21.91701\" \"16\"\n[2,] \"Sally\" \"1\" \"153860\" \"47.71114\" \"17.41023\" \"14\"\n[3,] \"Jane\"  \"4\" \"138180\" \"51.62570\" \"21.96218\" \"16\"\n[4,] \"Tom\"   \"2\" \"101360\" \"48.69179\" \"18.88657\" \"13\"\n[5,] \"Rick\"  \"2\" \"564000\" \"48.81529\" \"19.48836\" \"11\"\n[6,] \"Amy\"   \"3\" \"328830\" \"49.02871\" \"21.84143\" \" 9\""
  },
  {
    "objectID": "L1-2/lecture1_2.slides.html#standardisation-of-data",
    "href": "L1-2/lecture1_2.slides.html#standardisation-of-data",
    "title": "Lecture 1-2",
    "section": "Standardisation of data",
    "text": "Standardisation of data\nWhen analysing numerical data, it often happens that different variables are measured on scales of very different sizes. For example, in the above matrix the first question might ask one how many children one has and the second question might ask for one’s income in Rands. Clearly, the scale of possible values for the first question (between 0 and perhaps 15) is much smaller than for the second (between 0 and perhaps several million Rand). For reasons that will become clearer later on, this can cause enormous problems in some multivariate techniques by giving too much influence to the variables measured on larger scales. In order to put all variables on an equal footing, it is often necessary to standardise the data. Because several techniques require standardised data we consider it in this introductory chapter, but it is important to realise that not all the techniques need the data to be standardised. Moreover, in cases where all numerical variables are measured on the same scale (e.g.all on a 1 to 5 Likert rating scale) there will be no need to standardise either.\nThere are several different ways to standardise data, but the only one that we will use is to standardise the data so that each variable has a mean of zero and a standard deviation of one. In order to do this we carry out the following steps:\nWe will illustrate the standardisation of a data matrix using the following example. Suppose that information on three variables (income, number of children, and age) has been collected from five individuals. The data is contained in the following table.\n\n\n\nTable 1: Unstandardized Data with Summary Statistics\n\n\n\n\n\nPerson\nIncome\nNo Children\nAge\n\n\n\n\n\\(a\\)\n10000\n0\n40\n\n\n\\(b\\)\n0\n3\n23\n\n\n\\(c\\)\n300000\n2\n32\n\n\n\\(d\\)\n150000\n2\n35\n\n\n\\(e\\)\n1000000\n1\n58\n\n\n\\(\\bar{x}\\)\n292000\n1.6\n37.6\n\n\n\\(s\\)\n414210\n1.140\n12.973\n\n\n\n\n\n\nwhere we use the usual mathematical notation \\(\\bar{x}\\) to denote the mean and \\(s\\) to denote the standard deviation. Note that the variables are measured on very different scales. To standardise the data, we simply follow the steps above. For example, the standardised income of person \\(a\\) is given by \\[\\frac{10\\,000-292\\,000}{414\\,210}=-0.681\\] to three decimal places. Similarly the standardised number of children for person \\(d\\) is given by \\((2-1.6)/1.140=0.351\\). You can check for yourself that the new column means and standard deviations are all zero and one respectively. Since the mean of all the variables is zero, it is possible to see at a glance which observations are below average (those that are negative) and which are above average (those that are positive).\n\n\n\n\n\n\nThe relevance of standardising data may not seem clear to you at the moment. Just bear this section in mind as you continue through the notes and refer back to it when the issue of standardisation reappears.\n\nX &lt;- matrix (c(10000,0,300000,150000,1000000,0,3,2,2,1,40,23,32,35,58), \n             ncol=3,\n             dimnames=list(c(\"a\",\"b\",\"c\",\"d\",\"e\"), \n                           c(\"Income\",\"No Children\",\"Age\")))\n\nIn R we can create a matrix with the matrix() function. The values in the matrix are concatenated with the operator c(). Notice that the values needs to be entered column wise by default. The names for the two dimensions are specified by dimnames=list(“row names”, “column names”). Notice below that the row names appear to the left.\nThey are text, but are not part of the CONTENT of the matrix. The matrix X:5 × 3 contains only numeric values.\n\nX\n\n   Income No Children Age\na   10000           0  40\nb       0           3  23\nc  300000           2  32\nd  150000           2  35\ne 1000000           1  58\n\n\nTo calculate the means we apply to X, column wise (indicated by 2; 1 for row wise) the function mean().\n\nxbar &lt;- apply(X,2,mean)\nxbar\n\n     Income No Children         Age \n   292000.0         1.6        37.6 \n\n\nSimilarly, the function sd() is applied to each column to calculate the standard deviations.\n\ns &lt;- apply(X,2,sd)\ns\n\n      Income  No Children          Age \n4.142101e+05 1.140175e+00 1.297305e+01 \n\n\nAny numeric calculations can be performed by simply typing the expression at the R command prompt “&gt;”.\n\n(10000-292000)/414210\n\n[1] -0.6808141\n\n\nR has the ability to operate on a whole vector (or matrix) at once. Here the standardised values for Age is calculated by subtracting the mean from the values in column 2 and dividing resulting “column minus mean” by the standard deviation.\n\n(X[,2]-1.6)/1.14\n\n         a          b          c          d          e \n-1.4035088  1.2280702  0.3508772  0.3508772 -0.5263158 \n\n\nThe expressions above is simply for illustration purposes. The function scale() performs all the standardisation calculations in a single step. The output is again a matrix of size 5 × 3, but additional attributes are provided: first the mean called “scaled:center”, then the standard deviations called “scaled:scale”.\n\nscale(X)\n\n       Income No Children        Age\na -0.68081393  -1.4032928  0.1849989\nb -0.70495627   1.2278812 -1.1254101\nc  0.01931387   0.3508232 -0.4316641\nd -0.34282120   0.3508232 -0.2004155\ne  1.70927752  -0.5262348  1.5724908\nattr(,\"scaled:center\")\n     Income No Children         Age \n   292000.0         1.6        37.6 \nattr(,\"scaled:scale\")\n      Income  No Children          Age \n4.142101e+05 1.140175e+00 1.297305e+01 \n\n\nLet us begin by taking another look at the general \\(\\mathbf{X}\\) matrix from earlier in the chapter.\n\\[\n\\mathbf{X} = \\begin{bmatrix}\nx_{11} & x_{12} & x_{13} & x_{14} & x_{15} \\\\\nx_{21} & x_{22} & x_{23} & x_{24} & x_{25} \\\\\nx_{31} & x_{32} & x_{33} & x_{34} & x_{35} \\\\\nx_{41} & x_{42} & x_{43} & x_{44} & x_{45} \\\\\nx_{51} & x_{52} & x_{53} & x_{54} & x_{55} \\\\\nx_{61} & x_{62} & x_{63} & x_{64} & x_{65}\n\\end{bmatrix}\n\\]\nWe have already discussed the use of subscript notation, using the example of \\(x_{23}\\) – where the first subscript that the \\(x\\) we are interested in is in the second row and the third column of the data matrix. We can make this one step more general by referring to a general subscript \\(i\\) for the rows and \\(j\\) for the columns, to give \\(x_{ij}\\). In the same way that the \\(x\\)’s are just placeholders for value to come, so is \\(i\\) and so is \\(j\\). Thus, we can insert any value from 1 to 6 into \\(i\\) and any value from 1 to 5 into \\(j\\), and refer to a specific element of \\(\\mathbf{X}\\).\nThis becomes important because we don’t want to have to write out the whole matrix every time we want to do something with \\(\\mathbf{X}\\). The general \\(i\\) and \\(j\\) subscripts allow us to be much more concise. For example, suppose that the \\(x\\)’s are scores on 5 different tests. The score achieved by student \\(i\\) on test \\(j\\) is given by \\(x_{ij}\\). Suppose now that we want to find student 3’s average mark, which we label as \\(\\bar{x}_3\\). In order to do this we need to add up all 5 test scores and divide by 5. We could write \\[\\bar{x}_3=\\frac{x_{31}+x_{32}+x_{33}+x_{34}+x_{35}}{5}\\] or we could write \\[\\bar{x}_3=\\frac{1}{5}\\sum_{j=1}^5 x_{3j}\\] The summation sign \\((\\Sigma)\\) indicates that we add up all the terms following the sign, by letting \\(j\\) take each of the values in turn between the “limits of summation” (which are 1 and 5 respectively). In this case, it does not save much time or space to use summation notation, but in some cases it does. For example, if we now want refer to the average test score of any of the individuals in our data set, we have two choices: either use the “full” notation and write out all the averages \\[\\begin{align*}\n\\bar{x}_1&=(x_{11}+x_{12}+x_{13}+x_{14}+x_{15})/5 \\\\\n\\bar{x}_2&=(x_{21}+x_{22}+x_{23}+x_{24}+x_{25})/5 \\\\\n\\bar{x}_3&=(x_{31}+x_{32}+x_{33}+x_{34}+x_{35})/5 \\\\\n\\bar{x}_4&=(x_{41}+x_{42}+x_{43}+x_{44}+x_{45})/5 \\\\\n\\bar{x}_5&=(x_{51}+x_{52}+x_{53}+x_{54}+x_{55})/5 \\\\\n\\bar{x}_6&=(x_{61}+x_{62}+x_{63}+x_{64}+x_{65})/5\n\\end{align*}\\] or use the other general subscript \\(i\\) and write the average test score of person \\(i\\) is given by \\[\\bar{x}_i=\\frac{1}{5}\\sum_{j=1}^5 x_{ij}\\hspace{4em}(i=1,\\dots,6)\\] where the \\((i=1,\\dots,6)\\) part indicates that \\(i\\) can take on any value from 1 to 6. The compactness of the summation notation is clear to see. Now, suppose that we want to weight the different test scores differently. This is typically what happens in the calculation of a year mark. Suppose that the \\(x\\)’s are marks are 5 class tests, which are count \\(w_1\\), \\(w_2\\), \\(w_3\\), \\(w_4\\), \\(w_5\\) towards the final mark. Note that we have introduced another variable here, \\(w_j\\), to denote the weight attached to test \\(j\\). Then the year mark obtained by student \\(i\\) is given by\n\\[\\bar{x}_i=\\frac{w_1x_{i1}+w_2x_{i2}+w_3x_{i3}+w_4x_{i4}+w_5x_{i5}}{5}\\hspace{4em}(i=1,\\dots,6)\\] or we could just write \\[\\bar{x}_i=\\frac{1}{5}\\sum_{j=1}^5 w_jx_{ij}\\hspace{4em}(i=1,\\dots,6)\\] Note that the \\(w\\)’s only have a \\(j\\) subscript because they do not differ over students (the weighting is the same for all students) and so do not need an \\(i\\) subscript. Suppose that the weightings did differ over students (say because the weights needed to be adjusted if students miss a test for medical reasons). Then the \\(w\\)’s would be able to differ over students, we would need to include a subscript \\(i\\), and we would have an average mark given by \\[\\bar{x}_i=\\frac{1}{5}\\sum_{j=1}^5 w_{ij}x_{ij}\\hspace{4em}(i=1,\\dots,6)\\] Try writing out the average year mark for student 4 as a practice exercise. Finally, suppose that we want to work out a class average, labelled \\(C\\). For this we need to add together each student’s average mark \\(\\bar{x}_i\\) and then divide by the number of students in the class, 6. This can be written as\n\\[C=\\sum_{i=1}^6 \\bar{x}_i=\\frac{1}{6}\\sum_{i=1}^6 \\frac{1}{5}\\sum_{j=1}^5 w_{ij}x_{ij}=\\frac{1}{30}\\sum_{i=1}^6 \\sum_{j=1}^5 w_{ij}x_{ij}\\hspace{4em}(i=1,\\dots,6)\\] Once again, try writing this out in full as an exercise and to see the usefulness of the summarised notation!"
  },
  {
    "objectID": "L1-2/lecture1_2.slides.html#introduction-to-singular-value-decomposition-svd",
    "href": "L1-2/lecture1_2.slides.html#introduction-to-singular-value-decomposition-svd",
    "title": "Lecture 1-2",
    "section": "Introduction to Singular Value Decomposition (SVD)",
    "text": "Introduction to Singular Value Decomposition (SVD)\nThere is one result from matrix algebra that we will use extensively in the methods discussed in this course. Without going into the detail of the mathematics, the singular value decomposition (SVD) can be viewed as a “black box”. The results of the SVD can be stated in terms of matrices or individual elements of a matrix.\nMathematical Definition\nAny matrix \\(\\mathbf{X}\\) can be expressed as the product of three matrices \\(\\mathbf{U}\\), \\(\\mathbf{D}\\), and \\(\\mathbf{V}'\\). The \\(ij\\)-th element of \\(\\mathbf{X}\\), denoted as \\(x_{ij}\\), is expressed as:\n\\[x_{ij} = \\sum_{k=1}^r u_{ik} d_k v_{jk}\\]\nThe values \\(d_k\\) have only one subscript because the matrix \\(\\mathbf{D}\\) is a diagonal matrix with zeros for all off-diagonal elements.\nIf \\(\\mathbf{X}\\) is the matrix of standardised data (e.g., Income, Number of Children, and Age), then \\(r=3\\), and \\(\\mathbf{D}\\) will contain three diagonal values. These values in \\(\\mathbf{D}\\) are called the singular values. Singular values are always non-negative (\\(d_k \\ge 0\\)), and we order them such that \\(d_1 \\ge d_2 \\ge \\dots \\ge d_r\\)."
  },
  {
    "objectID": "L1-2/lecture1_2.slides.html#dimension-reduction",
    "href": "L1-2/lecture1_2.slides.html#dimension-reduction",
    "title": "Lecture 1-2",
    "section": "Dimension Reduction",
    "text": "Dimension Reduction\nThe SVD is the basis for approximating multivariate data by dimension reduction. Working with too many variables makes it difficult to discern interrelationships. We often seek a matrix \\(\\mathbf{X}^*\\) that is “simpler” than \\(\\mathbf{X}\\) but remains a good approximation.\nLeast Squares Approximation\nWe aim to find the least squares solution \\(\\mathbf{X}^*\\) that minimizes the sum of squared differences between the elements of \\(\\mathbf{X}\\) and \\(\\mathbf{X}^*\\):\n\\[\\min \\sum_{i} \\sum_{j} (x_{ij} - x_{ij}^*)^2\\]\nAccording to Huygens’ Principle, the approximation necessarily includes the centroid (mean), so we center the data matrix before approximating. If the data is already standardised, it is already centered.\nBest 2D approximation: \\(\\mathbf{X}^* = \\sum_{k=1}^2 u_{ik} d_k v_{jk}\\) Best 1D approximation: \\(\\mathbf{X}^* = u_{i1} d_1 v_{j1}\\)"
  },
  {
    "objectID": "L1-2/lecture1_2.slides.html#r-programming-exercise",
    "href": "L1-2/lecture1_2.slides.html#r-programming-exercise",
    "title": "Lecture 1-2",
    "section": "R Programming Exercise",
    "text": "R Programming Exercise\nUse the interactive console below to perform the SVD on a standardized dataset.\n\n# Step 1: Create the sample data matrix X\nX &lt;- matrix(c(50, 2, 35, \n              20, 4, 45, \n              40, 3, 30, \n              35, 3, 32, \n              60, 5, 25), \n            nrow = 5, byrow = TRUE)\ncolnames(X) &lt;- c(\"Income\", \"Children\", \"Age\")\n\n# Step 2: Standardise the matrix\nX.std &lt;- scale(X)\n\n# Step 3: Compute SVD\nres.svd &lt;- svd(X.std)\n\n# Extract components\nU &lt;- res.svd$u\nD &lt;- diag(res.svd$d)\nV &lt;- res.svd$v\n\n# Step 4: Construct the best 2-dimensional approximation (X.star)\n# We use only the first two columns/elements\nX.star &lt;- U[, 1:2] %*% D[1:2, 1:2] %*% t(V[, 1:2])\n\n# View results\nprint(\"Standardized Matrix:\")\n\n[1] \"Standardized Matrix:\"\n\nprint(X.std)\n\n          Income   Children        Age\n[1,]  0.59344243 -1.2278812  0.2151580\n[2,] -1.38469899  0.5262348  1.5598952\n[3,] -0.06593805 -0.3508232 -0.4572107\n[4,] -0.39562828 -0.3508232 -0.1882632\n[5,]  1.25282290  1.4032928 -1.1295793\nattr(,\"scaled:center\")\n  Income Children      Age \n    41.0      3.4     33.4 \nattr(,\"scaled:scale\")\n   Income  Children       Age \n15.165751  1.140175  7.436397 \n\nprint(\"2D Approximation (X.star):\")\n\n[1] \"2D Approximation (X.star):\"\n\nprint(X.star)\n\n            [,1]       [,2]       [,3]\n[1,]  0.26048350 -1.2630881 -0.1244430\n[2,] -1.51239121  0.5127327  1.4296557\n[3,]  0.21160127 -0.3214764 -0.1741349\n[4,] -0.09109489 -0.3186221  0.1223452\n[5,]  1.13140134  1.3904538 -1.2534230"
  },
  {
    "objectID": "L1-2/lecture1_2.slides.html#a-word-of-caution-on-practical-data-analysis",
    "href": "L1-2/lecture1_2.slides.html#a-word-of-caution-on-practical-data-analysis",
    "title": "Lecture 1-2",
    "section": "A word of caution on practical data analysis",
    "text": "A word of caution on practical data analysis\nOne of the main aims of this course is to put you in a position of being able to perform the multivariate statistical analysis of your own research projects, in whatever field this may be. Most of the examples used in these notes are themselves real-world studies, and so you will get some idea of some of the complexities involved in gathering and analysing data. Having said that, there is an obvious need in an introductory course like this one to choose data sets that work’ and that can be used to illustrate the techniques. We therefore do not discuss many of the practical difficulties which inevitably arise when doing your own original research. As a result when these difficulties arise when it comes to doing your own research, you may look back on this course and think why weren’t we taught that?’ Unfortunately, the kinds of problems that can arise are so varied and require such different solutions that it is not possible to teach in a course such as this one. As Bartholemew et al. put it, only when one has a clear idea of where one is going is it possible to know the important questions which arise”. However, the following broad areas should be borne in mind whenever conducting an original analysis.\n\n\n\n\n\n\nMissing Data\n\n\nMissing data can cause severe problems for many of the techniques we will consider. Most techniques will simply drop cases which possess missing data on any of the variables to be included in the analysis. When the number of variables is large, as is often the case in multivariate analyses, this can result in a substantial proportion of the sample being dropped. This proportion should always be noted early in the analysis. Another critical question to ask iswhy is the data missing?” and ’does the missing data introduce any bias into the results?” Often, it is the people with the most extreme views that turn up as missing data by refusing to answer certain questions, which is clearly biasing. Possible solutions are mean replacement or other imputation (replacement) techniques, but these are beyond the scope of this course.\n\n\n\n\n\n\n\n\n\nSample Sizes\n\n\nIt is a general rule that the bigger the model you fit, the greater the number of cases you need. In univariate analysis and simple hypothesis testing, the calculation ofrequired’\nsample sizes is reasonably straightforward, but in multivariate analysis there are only very rough guidelines where any exist at all. As a very rough guideline, most techniques require at least 10 respondents per parameter estimated. That means that in order to estimate a regression model with four independent variable, you need at least 50 respondents (not forgetting the constant term \\(\\beta_0\\), there are 5 parameters to be estimated). When sample sizes are small, one should be very careful about drawing strong conclusions. This is a particular problem in student research, where sample sizes are typically very small."
  },
  {
    "objectID": "L1-2/lecture1_2.html#software---but-then-why-r",
    "href": "L1-2/lecture1_2.html#software---but-then-why-r",
    "title": "Lecture 1-2",
    "section": "Software - But then why R?",
    "text": "Software - But then why R?\n\nBoth R and Python are free.\nR already has all of the statistics support because it was developed by statisticians for statisticians. A lot of statistical modelling research is conducted in R.\nPython was originally developed as a programming language for software development, DS tools (scikit-learn, pandas, numpy) were added on. Though the majority of DL research is done in Python, such as keras, PyTorch.\nR has Tidyverse, a set of packages that makes it easy to import, manipulate, visualise and report data.\nVery easy to generate dashboards using R Shiny.\nIt is the language I know the best, I know very little Python.\nPython and R programmers get inspired from each other, ie. Python’s plotnine inspired by R’s ggplot2, and R’s rvest by Python’s BeautifulSoup.\nYou can also use functions written in Python with python function in R.\nYou can run R code from Python with rp2 package, and you can run Python code from R using reticulate. R version of DL package Keras calls Python.\nThough, I do encourage you to learn Python as well. No harm in two languages.\n\nTake away: There is no winner, you are here to learn the skills, your focus should be on skills. If you can program in R, you can do it in any other language.\n\n\n\n\n\n\nReading\n\n\n\nPython vs. R for Data Science: What’s the Difference (by Richie Cotton)\nR vs Python for Data Science: The Winner is… (by Martijn Theuwissen) }"
  },
  {
    "objectID": "L1-2/lecture1_2.slides.html#software---but-then-why-r",
    "href": "L1-2/lecture1_2.slides.html#software---but-then-why-r",
    "title": "Lecture 1-2",
    "section": "Software - But then why R?",
    "text": "Software - But then why R?\n\nBoth R and Python are free.\nR already has all of the statistics support because it was developed by statisticians for statisticians. A lot of statistical modelling research is conducted in R.\nPython was originally developed as a programming language for software development, DS tools (scikit-learn, pandas, numpy) were added on. Though the majority of DL research is done in Python, such as keras, PyTorch.\nR has Tidyverse, a set of packages that makes it easy to import, manipulate, visualise and report data.\nVery easy to generate dashboards using R Shiny.\nIt is the language I know the best, I know very little Python.\nPython and R programmers get inspired from each other, ie. Python’s plotnine inspired by R’s ggplot2, and R’s rvest by Python’s BeautifulSoup.\nYou can also use functions written in Python with python function in R.\nYou can run R code from Python with rp2 package, and you can run Python code from R using reticulate. R version of DL package Keras calls Python.\nThough, I do encourage you to learn Python as well. No harm in two languages.\n\nTake away: There is no winner, you are here to learn the skills, your focus should be on skills. If you can program in R, you can do it in any other language.\n\n\n\n\n\n\n\nReading\n\n\nPython vs. R for Data Science: What’s the Difference (by Richie Cotton)\nR vs Python for Data Science: The Winner is… (by Martijn Theuwissen) }"
  },
  {
    "objectID": "L1-2/lecture1_2.html#datasets",
    "href": "L1-2/lecture1_2.html#datasets",
    "title": "Lecture 1-2",
    "section": "Datasets",
    "text": "Datasets\nIn this course, we will look at published MSc Data Science theses from OpenUCT and we will use publicly available datasets:\n\nZindi Competitions:\n\n– Zindi is the first data science competition platform in Africa. – Zindi hosts an entire data science ecosystem of scientists, engineers, academics, companies, NGOs, governments and institutions focused on solving Africa’s most pressing problems.\n\\vspace{.3cm}\n\n\\href{https://www.kaggle.com/competitions}{Kaggle competitions}: https://www.kaggle.com/competitions\n\\end{frame}\n\\begin{frame}{Dataset file formats}\nIf you are unfamiliar with any of this material, it is important to go back and revise in the first few weeks of the course."
  },
  {
    "objectID": "L1-2/lecture1_2.slides.html#datasets",
    "href": "L1-2/lecture1_2.slides.html#datasets",
    "title": "Lecture 1-2",
    "section": "Datasets",
    "text": "Datasets\nIn this course, we will look at published MSc Data Science theses from OpenUCT and we will use publicly available datasets:\n\nZindi Competitions:\n\n– Zindi is the first data science competition platform in Africa. – Zindi hosts an entire data science ecosystem of scientists, engineers, academics, companies, NGOs, governments and institutions focused on solving Africa’s most pressing problems.\n\\vspace{.3cm}\n\n\\href{https://www.kaggle.com/competitions}{Kaggle competitions}: https://www.kaggle.com/competitions\n\\end{frame}\n\\begin{frame}{Dataset file formats}\nIf you are unfamiliar with any of this material, it is important to go back and revise in the first few weeks of the course."
  },
  {
    "objectID": "L1-2/lecture1_2.html#datasets-and-variable-types",
    "href": "L1-2/lecture1_2.html#datasets-and-variable-types",
    "title": "Lecture 1-2",
    "section": "Datasets and Variable Types",
    "text": "Datasets and Variable Types\nIn this course, we will look at published MSc Data Science theses from OpenUCT (browse by department, type “Department of Statistical Sciences” in the search) and we will use publicly available datasets from various resources.\n\n\nDatasets\n\nZindi Competitions:\n\n– Zindi is the first data science competition platform in Africa.\n\nZindi hosts an entire data science ecosystem of scientists, engineers, academics, companies, NGOs, governments and institutions focused on solving Africa’s most pressing problems.\nKaggle competitions\n\n\n\n\nDataset types - Measurement levels\nAt this point it is probably worth spending a little time discussing different data types. There are two main types of data that we need to distinguish between: numerical variables, and categorical variables.\n\n\n\n\n\n\nImportant\n\n\n\nNumerical variables\nNumerical variables are measurements that can be recorded on a quantitative scale where the intervals between two values on the scale have some meaning. Essentially, this means that (a) the variable contains numbers rather than words or symbols, (b) the gaps between two numbers have some actual meaning. Examples of numerical variables are height, age, and number of children.\nCategorical variables are measurements of individuals in terms of groups or categories where the gap between categories have no intrinsic meaning. A typical example of a categorical variable is race, where the gap betweenblack’ and white’ has no proper interpretation, language, political affiliation, country of birth, and many other demographic variables.\n\n\n\nIt is vitally important to be able to distinguish between different data types because to a large extent these dictate what statistical techniques can be used. For example, it makes good sense to calculate the mean of a continuous variable but (as we have seen) no sense at all to calculate the mean of a categorical variable. The same idea extends to multivariate analysis. Some of the techniques we will look at work on correlation coefficients, which cannot be calculated for strictly categorical variables like race.\n\nOne further point on data types: some textbooks further divide numerical variables into ratio-scaled numerical variables and interval-scaled numerical variables; and divide categorical variables into ordinal categorical variables and nominal categorical variables. For the purposes of deciding which multivariate technique to use, this is an unnecessary detail and it is sufficient to know whether a variable is numerical or categorical. For the sake of completeness these additional terms are briefly described below. Ratio-scaled numerical variables are those that have a natural zero point (like age, height, and income). These are called “ratio-scaled” because the are not sensitive to units of measurement (if I am three times your height in meters I am also three times your height if it is measured in centimeters). This means that ratio-scaled variables have an arbitrary scale. Interval-scaled variables are still numeric but do not have a natural zero point (IQ, temperature in degrees Celcius, and most Likert-type rating scales are of this type). Interval-scaled variables therefore have an arbitrary zero point and an arbitrary scale. Ordinal categorical variables are those where the categories can be ordered even if the gaps between them cannot be interpreted (such as level of education, which can be ordered: none, primary-school, high-school, undergraduate degree, postgraduate degree). In contrast, the categories of a nominal categorical variable cannot be ordered in any meaningful way (such as race or language group). It is also common to further classify numerical variables as continuous if they can take on any intermediate value on the scale (e.g. height) or discrete if the values a variable can take on are limited in some way (e.g. number of children).\n\n\n\n\nData types\n\n\n\n\n\nDataset file formats\n\nRaw files, (.csv,.txt, .xlsx, .sav, etc.)\nDatabases (mySQL, MongoDB)\nAPIs (Twitter)\nand others…"
  },
  {
    "objectID": "L1-2/lecture1_2.slides.html#datasets-and-variable-types",
    "href": "L1-2/lecture1_2.slides.html#datasets-and-variable-types",
    "title": "Lecture 1-2",
    "section": "Datasets and Variable Types",
    "text": "Datasets and Variable Types\nIn this course, we will look at published MSc Data Science theses from OpenUCT (browse by department, type “Department of Statistical Sciences” in the search) and we will use publicly available datasets from various resources."
  },
  {
    "objectID": "L1-2/lecture1_2.slides.html#types-of-eda",
    "href": "L1-2/lecture1_2.slides.html#types-of-eda",
    "title": "Lecture 1-2",
    "section": "Types of EDA",
    "text": "Types of EDA\nThere are broadly two types of EDA:\n\nUnivariate\nMultivariate\n\nMost research in the business and social sciences makes use of some kind of multivariate analysis. Research that considers only one variable at a time (a univariate analysis) can provide useful information – for example, about the average rate of inflation over time, the variability of a particular share’s return, or the relative proportion of the population that hold a certain opinion – but it is usually in the consideration of relationships between two or more variables that the most interesting and useful information is to be found. For example, what other variables are related to increases in the inflation rate or the rise in the price of a particular share? Is it interest rates? Foreign exchange rates? And what causes people to prefer one opinion over another? Is it their education level? Income? The newspaper they read? Simply put, any analysis that considers the relationship between two or more variables is a multivariate analysis."
  },
  {
    "objectID": "L1-2/lecture1_2.slides.html#following-tukeys-steps",
    "href": "L1-2/lecture1_2.slides.html#following-tukeys-steps",
    "title": "Lecture 1-2",
    "section": "Following Tukey’s Steps",
    "text": "Following Tukey’s Steps\nTukey’s EDA book provides techniques and advice about how to explore data.\n\nThe approach of EDA is detective in character, it is a search for clues. Some of the clues may be misleading, but some will lead to discoveries.\nTukey favors simplicity because simple statements are clear.\nTukey favors clear visual displays of quantitative facts.\nTukey likes precision, it is far better to be able to say some response measure is a linear function of a particular stimulus variable than to say it increases with the stimulus variable.\nTukey favors depth of analysis. It is always good to look at the residuals.\nTukey values accuracy. A misplaced decimal vs a misplaced digit.\nTukey values replicability of summary observations in situations containing aberrant observations."
  },
  {
    "objectID": "L1-2/lecture1_2.slides.html#eda-methods---summary-statistics",
    "href": "L1-2/lecture1_2.slides.html#eda-methods---summary-statistics",
    "title": "Lecture 1-2",
    "section": "EDA methods - Summary Statistics",
    "text": "EDA methods - Summary Statistics\nWith summary statistics, our aim is to reduce a large data set to a few numbers which will help us understand the important features of the data.\n\nCompute a few “key” numbers: 5 number summaries.\nLet’s begin with the concept of ranked data.\nIn a sample of size n, the smallest number has a rank of 1; the second smallest number has a rank of 2; …. ; the largest number has a rank of n.\n\n\n\\(x_1, x_2, x_3, ..., x_n\\) and \\(x_{(r)}\\) is the number with rank \\(r\\).\n\nFrequency distributions\nMeasures of central tendency and variability\nScale transformations\nSmoothing techniques\nAnalysis of tables"
  },
  {
    "objectID": "L1-2/lecture1_2.slides.html#eda-methods---plots",
    "href": "L1-2/lecture1_2.slides.html#eda-methods---plots",
    "title": "Lecture 1-2",
    "section": "EDA methods - Plots",
    "text": "EDA methods - Plots\nTukey particularly emphasizes the value of graphs for discovery. Tukey’s approach to data analysis is highly visual and he has numerous suggestions for graphical displays. Tukey emphasizes the value of graphs for the following:\n\nGraphs can be used to store quantitative data,\nGraphs can be used to communicate conclusions,\nGraphs can be used to discover new information.\n\nSome types of plots are better for one purpose, others are better for another."
  },
  {
    "objectID": "L1-2/lecture1_2.html#types-of-eda",
    "href": "L1-2/lecture1_2.html#types-of-eda",
    "title": "Lecture 1-2",
    "section": "Types of EDA",
    "text": "Types of EDA\nThere are broadly two types of EDA:\n\nUnivariate\nMultivariate\n\nMost research in the business and social sciences makes use of some kind of multivariate analysis. Research that considers only one variable at a time (a univariate analysis) can provide useful information – for example, about the average rate of inflation over time, the variability of a particular share’s return, or the relative proportion of the population that hold a certain opinion – but it is usually in the consideration of relationships between two or more variables that the most interesting and useful information is to be found. For example, what other variables are related to increases in the inflation rate or the rise in the price of a particular share? Is it interest rates? Foreign exchange rates? And what causes people to prefer one opinion over another? Is it their education level? Income? The newspaper they read? Simply put, any analysis that considers the relationship between two or more variables is a multivariate analysis."
  },
  {
    "objectID": "L1-2/lecture1_2.html#following-tukeys-steps",
    "href": "L1-2/lecture1_2.html#following-tukeys-steps",
    "title": "Lecture 1-2",
    "section": "Following Tukey’s Steps",
    "text": "Following Tukey’s Steps\nTukey’s EDA book provides techniques and advice about how to explore data.\n\nThe approach of EDA is detective in character, it is a search for clues. Some of the clues may be misleading, but some will lead to discoveries.\nTukey favors simplicity because simple statements are clear.\nTukey favors clear visual displays of quantitative facts.\nTukey likes precision, it is far better to be able to say some response measure is a linear function of a particular stimulus variable than to say it increases with the stimulus variable.\nTukey favors depth of analysis. It is always good to look at the residuals.\nTukey values accuracy. A misplaced decimal vs a misplaced digit.\nTukey values replicability of summary observations in situations containing aberrant observations."
  },
  {
    "objectID": "L1-2/lecture1_2.html#eda-methods---summary-statistics",
    "href": "L1-2/lecture1_2.html#eda-methods---summary-statistics",
    "title": "Lecture 1-2",
    "section": "EDA methods - Summary Statistics",
    "text": "EDA methods - Summary Statistics\nWith summary statistics, our aim is to reduce a large data set to a few numbers which will help us understand the important features of the data.\n\nCompute a few “key” numbers: 5 number summaries.\nLet’s begin with the concept of ranked data.\nIn a sample of size n, the smallest number has a rank of 1; the second smallest number has a rank of 2; …. ; the largest number has a rank of n.\n\n\n\\(x_1, x_2, x_3, ..., x_n\\) and \\(x_{(r)}\\) is the number with rank \\(r\\).\n\nFrequency distributions\nMeasures of central tendency and variability\nScale transformations\nSmoothing techniques\nAnalysis of tables"
  },
  {
    "objectID": "L1-2/lecture1_2.html#eda-methods---plots",
    "href": "L1-2/lecture1_2.html#eda-methods---plots",
    "title": "Lecture 1-2",
    "section": "EDA methods - Plots",
    "text": "EDA methods - Plots\nTukey particularly emphasizes the value of graphs for discovery. Tukey’s approach to data analysis is highly visual and he has numerous suggestions for graphical displays. Tukey emphasizes the value of graphs for the following:\n\nGraphs can be used to store quantitative data,\nGraphs can be used to communicate conclusions,\nGraphs can be used to discover new information.\n\nSome types of plots are better for one purpose, others are better for another.\n\n\nDistribution of a Single Quantitative Variable\nTukey’s novel distribution tools: - Stem and Leaf: The measures Tukey proposes involve no arithmetic, only counting. - Histogram: One should note, * its height, * where it is centered, * how spread out it is, * whether it is asymmetric, * whether there are any discontinuities.\n\nBox-Whisker Plots: These plots show medians, quartiles, and two extreme values in a format that is easy to grasp quickly. Very powerful when comparing several frequency distributions.\nQ-Q plots\n\n\n\n\nVisual Display of a Single Qualitative Variable\n\nFrequency distribution\nPie chart\nBar chart\n\nVisualising plays an important role in exploring your data, and you would know that Tukey favours analysis of data with four-color pen, graph paper, few tables etc.:\n\n\n\ngraph paper\n\n\nThough we will use R and its functions for this purpose:\n\n\n\nStandardisation of data\nWhen analysing numerical data, it often happens that different variables are measured on scales of very different sizes. For example, in the above matrix the first question might ask one how many children one has and the second question might ask for one’s income in Rands. Clearly, the scale of possible values for the first question (between 0 and perhaps 15) is much smaller than for the second (between 0 and perhaps several million Rand). For reasons that will become clearer later on, this can cause enormous problems in some multivariate techniques by giving too much influence to the variables measured on larger scales. In order to put all variables on an equal footing, it is often necessary to standardise the data. Because several techniques require standardised data we consider it in this introductory chapter, but it is important to realise that not all the techniques need the data to be standardised. Moreover, in cases where all numerical variables are measured on the same scale (e.g.all on a 1 to 5 Likert rating scale) there will be no need to standardise either.\nThere are several different ways to standardise data, but the only one that we will use is to standardise the data so that each variable has a mean of zero and a standard deviation of one. In order to do this we carry out the following steps:\nWe will illustrate the standardisation of a data matrix using the following example. Suppose that information on three variables (income, number of children, and age) has been collected from five individuals. The data is contained in the following table.\n\n\n\nTable 1: Unstandardized Data with Summary Statistics\n\n\n\n\n\nPerson\nIncome\nNo Children\nAge\n\n\n\n\n\\(a\\)\n10000\n0\n40\n\n\n\\(b\\)\n0\n3\n23\n\n\n\\(c\\)\n300000\n2\n32\n\n\n\\(d\\)\n150000\n2\n35\n\n\n\\(e\\)\n1000000\n1\n58\n\n\n\\(\\bar{x}\\)\n292000\n1.6\n37.6\n\n\n\\(s\\)\n414210\n1.140\n12.973\n\n\n\n\n\n\nwhere we use the usual mathematical notation \\(\\bar{x}\\) to denote the mean and \\(s\\) to denote the standard deviation. Note that the variables are measured on very different scales. To standardise the data, we simply follow the steps above. For example, the standardised income of person \\(a\\) is given by \\[\\frac{10\\,000-292\\,000}{414\\,210}=-0.681\\] to three decimal places. Similarly the standardised number of children for person \\(d\\) is given by \\((2-1.6)/1.140=0.351\\). You can check for yourself that the new column means and standard deviations are all zero and one respectively. Since the mean of all the variables is zero, it is possible to see at a glance which observations are below average (those that are negative) and which are above average (those that are positive).\n\nkableExtra::kable(std_data)\n\n\n\nTable 2: Standardised Data\n\n\n\n\n\n\n\n\n\n\n\n\nPerson\nIncome\nN.Children\nAge\n\n\n\n\na\n-0.681\n-1.403\n0.185\n\n\nb\n-0.705\n1.228\n-1.125\n\n\nc\n0.019\n0.351\n-0.432\n\n\nd\n-0.343\n0.351\n-0.200\n\n\ne\n1.709\n-0.526\n1.572\n\n\n\n\n\n\n\n\nThe relevance of standardising data may not seem clear to you at the moment. Just bear this section in mind as you continue through the notes and refer back to it when the issue of standardisation reappears.\n\nX &lt;- matrix (c(10000,0,300000,150000,1000000,0,3,2,2,1,40,23,32,35,58), \n             ncol=3,\n             dimnames=list(c(\"a\",\"b\",\"c\",\"d\",\"e\"), \n                           c(\"Income\",\"No Children\",\"Age\")))\n\nIn R we can create a matrix with the matrix() function. The values in the matrix are concatenated with the operator c(). Notice that the values needs to be entered column wise by default. The names for the two dimensions are specified by dimnames=list(“row names”, “column names”). Notice below that the row names appear to the left.\nThey are text, but are not part of the CONTENT of the matrix. The matrix X:5 × 3 contains only numeric values.\n\nX\n\n   Income No Children Age\na   10000           0  40\nb       0           3  23\nc  300000           2  32\nd  150000           2  35\ne 1000000           1  58\n\n\nTo calculate the means we apply to X, column wise (indicated by 2; 1 for row wise) the function mean().\n\nxbar &lt;- apply(X,2,mean)\nxbar\n\n     Income No Children         Age \n   292000.0         1.6        37.6 \n\n\nSimilarly, the function sd() is applied to each column to calculate the standard deviations.\n\ns &lt;- apply(X,2,sd)\ns\n\n      Income  No Children          Age \n4.142101e+05 1.140175e+00 1.297305e+01 \n\n\nAny numeric calculations can be performed by simply typing the expression at the R command prompt “&gt;”.\n\n(10000-292000)/414210\n\n[1] -0.6808141\n\n\nR has the ability to operate on a whole vector (or matrix) at once. Here the standardised values for Age is calculated by subtracting the mean from the values in column 2 and dividing resulting “column minus mean” by the standard deviation.\n\n(X[,2]-1.6)/1.14\n\n         a          b          c          d          e \n-1.4035088  1.2280702  0.3508772  0.3508772 -0.5263158 \n\n\nThe expressions above is simply for illustration purposes. The function scale() performs all the standardisation calculations in a single step. The output is again a matrix of size 5 × 3, but additional attributes are provided: first the mean called “scaled:center”, then the standard deviations called “scaled:scale”.\n\nscale(X)\n\n       Income No Children        Age\na -0.68081393  -1.4032928  0.1849989\nb -0.70495627   1.2278812 -1.1254101\nc  0.01931387   0.3508232 -0.4316641\nd -0.34282120   0.3508232 -0.2004155\ne  1.70927752  -0.5262348  1.5724908\nattr(,\"scaled:center\")\n     Income No Children         Age \n   292000.0         1.6        37.6 \nattr(,\"scaled:scale\")\n      Income  No Children          Age \n4.142101e+05 1.140175e+00 1.297305e+01 \n\n\n\n\nTransformations - Which transformations and Why?\nVery often it is more convenient to look at some transform of the original variable. If the distribution is far from symmetrical, one end of the distribution will be too crowded to permit careful inspection.\nTukey deals extensively with scale transformations. He gives three main reasons for transformations:\n\nA transformation may be selected to produce a symmetrical distribution,\nA transformation may increase the similarity of the spread of the different sets of numbers,\nA transformation may straighten out a line.\n\n\n\n\nTypes of transformations\n\n\n\n\n\n\nTransformations\n\n\n\nMany statistical techniques assume that data are normally distributed. Although it is again beyond the scope of this course, it is often possible to transform data that is not normally distributed into something that is normally distributed by using some kind of transforming function. Taking the logarithm of a set of numbers, for example, often works, as does taking the square (both of these transformations work by sucking in’ the tails of the non-normal distributions). Where transformations do not help, the analyst must make a decision about whether the data is approximately normal’ or ‘normal enough’ to continue, or whether it is necessary to use other methods (like non-parametric statistics, which tend to be harder to use but do not make any distributional assumptions).\n\n\nThe transformations discussed range on:\n\n\\(x^n\\)\n\\(x^{n-1}\\)\n\\(x^2\\)\n\\(\\log x\\)\n\\(-\\frac{1}{x}\\)\n\\(-\\frac{1}{x^2}\\)\n\\(-\\frac{1}{x^n}\\)\n\nOther dependent variables, e.g. counts and latencies, are occasionally transformed by taking the square root, the logarithm, or the reciprocal."
  },
  {
    "objectID": "L1-2/lecture1_2.html#r-examples",
    "href": "L1-2/lecture1_2.html#r-examples",
    "title": "Lecture 1-2",
    "section": "R Examples",
    "text": "R Examples\nThe things that we are collecting data from (which could be people, shares, countries, animal species, songs \\(\\dots\\) anything you can collect data on) are called cases or responses. These appear as separate rows in the data matrix. The pieces of information that we use to describe each case are called variables or attributes and these appear in the columns of the data matrix. The \\(x\\)’s, remember, are simply placeholders for values to come. Specifically, the values to come may be numbers, or they may be words. It is perfectly allowable for the first column of \\(x\\)’s to be, for example, the first names of each person, e.g. \\(x_{11}=\\text{Iris}\\). Of course, this will affect the type of analysis we can do later on that variable (for example, it wouldn’t make sense to calculate a mean’ first name).\n\nExample 1 - Female headed households in SA\nWomxn in Big Data South Africa: Female-Headed Households in South Africa competition\nThe datasets are provided in a .csv file format, test.csv, train.csv, variable_descriptions.csv.\nThe target variable of interest is the percentage of households per ward that are both female-headed and earn an annual income that is below R19,600 (approximately $2,300 USD in 2011).\n\nTrain &lt;- read.csv(\"../Datasets/Woman/Train.csv\", header = TRUE)\nstr(Train)\n\n'data.frame':   2822 obs. of  63 variables:\n $ ward             : chr  \"41601001: Ward 1\" \"41601002: Ward 2\" \"41601003: Ward 3\" \"41601004: Ward 4\" ...\n $ total_households : num  1674 1737 2404 1741 1731 ...\n $ total_individuals: num  5888 6735 7273 5734 6657 ...\n $ target           : num  16.8 21.5 10.9 23.1 13.7 ...\n $ dw_00            : num  0.934 0.697 0.811 0.66 0.951 ...\n $ dw_01            : num  0.000846 0.001253 0.004517 0 0.000655 ...\n $ dw_02            : num  0.00549 0.0044 0.00889 0.00613 0.00147 ...\n $ dw_03            : num  0.000676 0 0.003986 0 0.000598 ...\n $ dw_04            : num  0 0.002301 0.007735 0.000813 0.006999 ...\n $ dw_05            : num  0.001372 0.001323 0.000956 0.037245 0.000818 ...\n $ dw_06            : num  0.00575 0.00757 0.00669 0.00526 0.00498 ...\n $ dw_07            : num  0.03147 0.12355 0.02263 0.06891 0.00915 ...\n $ dw_08            : num  0.00808 0.15191 0.1299 0.21879 0.01538 ...\n $ dw_09            : num  0.00282 0.00149 0 0 0.00869 ...\n $ dw_10            : num  0.00143 0.00125 0 0 0 ...\n $ dw_11            : num  0.008224 0.00801 0.00415 0.002947 0.000673 ...\n $ dw_12            : int  0 0 0 0 0 0 0 0 0 0 ...\n $ dw_13            : int  0 0 0 0 0 0 0 0 0 0 ...\n $ psa_00           : num  0.26 0.29 0.186 0.281 0.197 ...\n $ psa_01           : num  0.608 0.55 0.677 0.593 0.518 ...\n $ psa_02           : num  0.000188 0 0.000489 0.000579 0.000989 ...\n $ psa_03           : num  0.01002 0.02134 0.02131 0.00725 0.00515 ...\n $ psa_04           : num  0.122 0.139 0.115 0.118 0.28 ...\n $ stv_00           : num  0.2835 0.1036 0.1658 0.0878 0.346 ...\n $ stv_01           : num  0.717 0.896 0.834 0.912 0.654 ...\n $ car_00           : num  0.274 0.145 0.272 0.128 0.405 ...\n $ car_01           : num  0.726 0.855 0.728 0.872 0.595 ...\n $ lln_00           : num  0.1188 0.0669 0.1 0.0292 0.1336 ...\n $ lln_01           : num  0.881 0.933 0.9 0.971 0.866 ...\n $ lan_00           : num  0.833 0.88 0.566 0.744 0.423 ...\n $ lan_01           : num  0.01234 0.00845 0.01599 0.00653 0.01435 ...\n $ lan_02           : num  0.001923 0.000328 0.001566 0.001188 0.000842 ...\n $ lan_03           : num  0.0509 0.0112 0.1113 0.0864 0.1219 ...\n $ lan_04           : num  0 0.000842 0.004795 0.006735 0.007027 ...\n $ lan_05           : num  0.000564 0.001759 0.002552 0.002308 0.002613 ...\n $ lan_06           : num  0.0761 0.0324 0.1481 0.1032 0.1474 ...\n $ lan_07           : num  0.00637 0.03084 0.13969 0.03828 0.08171 ...\n $ lan_08           : num  0.00366 0.00165 0.00317 0.00308 0.00304 ...\n $ lan_09           : num  0.000375 0.001308 0.000165 0.000582 0.000169 ...\n $ lan_10           : num  0.000372 0.000994 0.000779 0 0.000643 ...\n $ lan_11           : num  0.004943 0 0.001692 0.000197 0.001201 ...\n $ lan_12           : num  0.00272 0.00244 0.00251 0.00744 0.00428 ...\n $ lan_13           : int  0 0 0 0 0 0 0 0 0 0 ...\n $ lan_14           : num  0.006793 0.028061 0.0022 0.000174 0.192272 ...\n $ pg_00            : num  0.357 0.698 0.672 0.728 0.753 ...\n $ pg_01            : num  0.563 0.278 0.154 0.264 0.13 ...\n $ pg_02            : num  0.00426 0.0037 0.00218 0.00181 0.00452 ...\n $ pg_03            : num  0.072996 0.015835 0.167494 0.000956 0.106953 ...\n $ pg_04            : num  0.00212 0.00404 0.00365 0.00539 0.00538 ...\n $ lgt_00           : num  0.919 0.959 0.826 0.986 0.957 ...\n $ pw_00            : num  0.743 0.309 0.323 0.677 0.771 ...\n $ pw_01            : num  0.214 0.577 0.483 0.314 0.195 ...\n $ pw_02            : num  0.01997 0.01895 0.08301 0.00269 0.0097 ...\n $ pw_03            : num  0.00285 0.01457 0.05756 0 0.00486 ...\n $ pw_04            : num  0.007537 0.057127 0.010358 0.000669 0.00129 ...\n $ pw_05            : num  0 0.019092 0.001421 0 0.000673 ...\n $ pw_06            : num  0.01293 0.00413 0.04088 0.00501 0.01763 ...\n $ pw_07            : int  0 0 0 0 0 0 0 0 0 0 ...\n $ pw_08            : int  0 0 0 0 0 0 0 0 0 0 ...\n $ ADM4_PCODE       : chr  \"ZA4161001\" \"ZA4161002\" \"ZA4161003\" \"ZA4161004\" ...\n $ lat              : num  -29.7 -29.1 -29.1 -29.4 -29.4 ...\n $ lon              : num  24.7 24.8 25.1 24.9 25.3 ...\n $ NL               : num  0.292 3.208 0 2.039 0 ...\n\nsummary(Train)\n\n     ward           total_households total_individuals     target     \n Length:2822        Min.   :    1    Min.   :  402     Min.   : 0.00  \n Class :character   1st Qu.: 1779    1st Qu.: 7071     1st Qu.:16.75  \n Mode  :character   Median : 2398    Median : 9367     Median :24.16  \n                    Mean   : 3665    Mean   :12869     Mean   :24.51  \n                    3rd Qu.: 3987    3rd Qu.:14241     3rd Qu.:32.23  \n                    Max.   :39685    Max.   :91717     Max.   :55.53  \n     dw_00            dw_01              dw_02              dw_03          \n Min.   :0.0000   Min.   :0.000000   Min.   :0.000000   Min.   :0.0000000  \n 1st Qu.:0.5942   1st Qu.:0.002895   1st Qu.:0.002407   1st Qu.:0.0000000  \n Median :0.7668   Median :0.010425   Median :0.005762   Median :0.0008066  \n Mean   :0.7122   Mean   :0.092616   Mean   :0.032043   Mean   :0.0060567  \n 3rd Qu.:0.8817   3rd Qu.:0.068209   3rd Qu.:0.027913   3rd Qu.:0.0025383  \n Max.   :0.9950   Max.   :0.931489   Max.   :0.951806   Max.   :0.2642393  \n     dw_04               dw_05               dw_06              dw_07         \n Min.   :0.0000000   Min.   :0.0000000   Min.   :0.000000   Min.   :0.000000  \n 1st Qu.:0.0000000   1st Qu.:0.0000000   1st Qu.:0.002716   1st Qu.:0.004716  \n Median :0.0006069   Median :0.0008654   Median :0.008639   Median :0.016295  \n Mean   :0.0086655   Mean   :0.0062888   Mean   :0.022374   Mean   :0.039296  \n 3rd Qu.:0.0022246   3rd Qu.:0.0030272   3rd Qu.:0.025218   3rd Qu.:0.048731  \n Max.   :0.3920853   Max.   :0.4359115   Max.   :0.412936   Max.   :0.455815  \n     dw_08              dw_09               dw_10               dw_11         \n Min.   :0.000000   Min.   :0.0000000   Min.   :0.0000000   Min.   :0.000000  \n 1st Qu.:0.002888   1st Qu.:0.0002329   1st Qu.:0.0000000   1st Qu.:0.001991  \n Median :0.014991   Median :0.0017552   Median :0.0003909   Median :0.004092  \n Mean   :0.064586   Mean   :0.0068641   Mean   :0.0011121   Mean   :0.007902  \n 3rd Qu.:0.074748   3rd Qu.:0.0065068   3rd Qu.:0.0010425   3rd Qu.:0.007803  \n Max.   :0.798479   Max.   :0.2828433   Max.   :0.0687517   Max.   :1.000000  \n     dw_12       dw_13       psa_00           psa_01        \n Min.   :0   Min.   :0   Min.   :0.0000   Min.   :0.001293  \n 1st Qu.:0   1st Qu.:0   1st Qu.:0.2556   1st Qu.:0.467217  \n Median :0   Median :0   Median :0.3017   Median :0.540874  \n Mean   :0   Mean   :0   Mean   :0.3113   Mean   :0.526568  \n 3rd Qu.:0   3rd Qu.:0   3rd Qu.:0.3712   3rd Qu.:0.586087  \n Max.   :0   Max.   :0   Max.   :0.5616   Max.   :0.852493  \n     psa_02              psa_03            psa_04            stv_00      \n Min.   :0.0000000   Min.   :0.00000   Min.   :0.04279   Min.   :0.0000  \n 1st Qu.:0.0001326   1st Qu.:0.01698   1st Qu.:0.11014   1st Qu.:0.0982  \n Median :0.0003381   Median :0.02705   Median :0.12576   Median :0.1728  \n Mean   :0.0005410   Mean   :0.03369   Mean   :0.12793   Mean   :0.2259  \n 3rd Qu.:0.0006835   3rd Qu.:0.04350   3rd Qu.:0.13973   3rd Qu.:0.3034  \n Max.   :0.0194420   Max.   :0.26738   Max.   :0.99871   Max.   :0.8405  \n     stv_01           car_00           car_01            lln_00       \n Min.   :0.1595   Min.   :0.0000   Min.   :0.04133   Min.   :0.00000  \n 1st Qu.:0.6966   1st Qu.:0.1310   1st Qu.:0.71851   1st Qu.:0.01732  \n Median :0.8272   Median :0.1780   Median :0.82197   Median :0.04014  \n Mean   :0.7741   Mean   :0.2503   Mean   :0.74969   Mean   :0.09764  \n 3rd Qu.:0.9018   3rd Qu.:0.2815   3rd Qu.:0.86902   3rd Qu.:0.12087  \n Max.   :1.0000   Max.   :0.9587   Max.   :1.00000   Max.   :0.76261  \n     lln_01           lan_00             lan_01             lan_02        \n Min.   :0.2374   Min.   :0.000000   Min.   :0.000000   Min.   :0.000000  \n 1st Qu.:0.8791   1st Qu.:0.002842   1st Qu.:0.009433   1st Qu.:0.004081  \n Median :0.9599   Median :0.007914   Median :0.017589   Median :0.008956  \n Mean   :0.9024   Mean   :0.097603   Mean   :0.058684   Mean   :0.029416  \n 3rd Qu.:0.9827   3rd Qu.:0.059327   3rd Qu.:0.036612   3rd Qu.:0.015081  \n Max.   :1.0000   Max.   :0.979246   Max.   :0.939549   Max.   :0.895365  \n     lan_03             lan_04            lan_05             lan_06        \n Min.   :0.000000   Min.   :0.00000   Min.   :0.000000   Min.   :0.000000  \n 1st Qu.:0.001647   1st Qu.:0.01034   1st Qu.:0.001675   1st Qu.:0.002681  \n Median :0.008835   Median :0.05253   Median :0.003986   Median :0.017154  \n Mean   :0.039983   Mean   :0.28432   Mean   :0.116773   Mean   :0.108053  \n 3rd Qu.:0.039564   3rd Qu.:0.56850   3rd Qu.:0.055631   3rd Qu.:0.066745  \n Max.   :0.852927   Max.   :0.98616   Max.   :0.978779   Max.   :0.981207  \n     lan_07             lan_08             lan_09              lan_10         \n Min.   :0.000000   Min.   :0.000000   Min.   :0.0000000   Min.   :0.0000000  \n 1st Qu.:0.003906   1st Qu.:0.001675   1st Qu.:0.0002974   1st Qu.:0.0002999  \n Median :0.008403   Median :0.003045   Median :0.0012675   Median :0.0012002  \n Mean   :0.130673   Mean   :0.004621   Mean   :0.0243186   Mean   :0.0242625  \n 3rd Qu.:0.065157   3rd Qu.:0.005782   3rd Qu.:0.0065378   3rd Qu.:0.0052470  \n Max.   :0.963219   Max.   :0.034234   Max.   :0.9812332   Max.   :0.9828445  \n     lan_11             lan_12             lan_13      lan_14         \n Min.   :0.000000   Min.   :0.000000   Min.   :0   Min.   :0.0000000  \n 1st Qu.:0.000495   1st Qu.:0.002589   1st Qu.:0   1st Qu.:0.0000000  \n Median :0.003261   Median :0.006394   Median :0   Median :0.0001459  \n Mean   :0.053985   Mean   :0.012809   Mean   :0   Mean   :0.0145029  \n 3rd Qu.:0.029783   3rd Qu.:0.013722   3rd Qu.:0   3rd Qu.:0.0121078  \n Max.   :0.991674   Max.   :0.367785   Max.   :0   Max.   :0.9984484  \n     pg_00             pg_01              pg_02               pg_03          \n Min.   :0.01105   Min.   :0.000000   Min.   :0.0000000   Min.   :0.0000000  \n 1st Qu.:0.87528   1st Qu.:0.001015   1st Qu.:0.0008769   1st Qu.:0.0004514  \n Median :0.98975   Median :0.003124   Median :0.0017966   Median :0.0012081  \n Mean   :0.86214   Mean   :0.040938   Mean   :0.0187979   Mean   :0.0744293  \n 3rd Qu.:0.99562   3rd Qu.:0.012582   3rd Qu.:0.0048827   3rd Qu.:0.0418406  \n Max.   :1.00000   Max.   :0.969519   Max.   :0.9395640   Max.   :0.9405628  \n     pg_04               lgt_00             pw_00             pw_01       \n Min.   :0.0000000   Min.   :0.001692   Min.   :0.00000   Min.   :0.0000  \n 1st Qu.:0.0006644   1st Qu.:0.796471   1st Qu.:0.08764   1st Qu.:0.1113  \n Median :0.0016958   Median :0.914061   Median :0.27800   Median :0.3021  \n Mean   :0.0036926   Mean   :0.836432   Mean   :0.35969   Mean   :0.3297  \n 3rd Qu.:0.0041264   3rd Qu.:0.964334   3rd Qu.:0.58295   3rd Qu.:0.5088  \n Max.   :0.3678423   Max.   :1.000000   Max.   :0.99591   Max.   :0.9376  \n     pw_02              pw_03              pw_04               pw_05          \n Min.   :0.000000   Min.   :0.000000   Min.   :0.0000000   Min.   :0.0000000  \n 1st Qu.:0.008673   1st Qu.:0.002099   1st Qu.:0.0007147   1st Qu.:0.0001595  \n Median :0.069065   Median :0.016496   Median :0.0051637   Median :0.0014590  \n Mean   :0.127555   Mean   :0.041589   Mean   :0.0196551   Mean   :0.0110081  \n 3rd Qu.:0.183384   3rd Qu.:0.058626   3rd Qu.:0.0250545   3rd Qu.:0.0094322  \n Max.   :1.000000   Max.   :0.327393   Max.   :0.3067867   Max.   :0.2282606  \n     pw_06              pw_07       pw_08    ADM4_PCODE             lat        \n Min.   :0.000000   Min.   :0   Min.   :0   Length:2822        Min.   :-32.49  \n 1st Qu.:0.005217   1st Qu.:0   1st Qu.:0   Class :character   1st Qu.:-28.57  \n Median :0.025165   Median :0   Median :0   Mode  :character   Median :-26.55  \n Mean   :0.110818   Mean   :0   Mean   :0                      Mean   :-26.88  \n 3rd Qu.:0.116638   3rd Qu.:0   3rd Qu.:0                      3rd Qu.:-25.57  \n Max.   :0.961522   Max.   :0   Max.   :0                      Max.   :-22.33  \n      lon              NL        \n Min.   :16.76   Min.   : 0.000  \n 1st Qu.:27.71   1st Qu.: 3.033  \n Median :28.96   Median : 9.206  \n Mean   :28.67   Mean   :17.438  \n 3rd Qu.:30.44   3rd Qu.:26.891  \n Max.   :32.86   Max.   :63.000  \n\n\nNow we will explore this dataset.\nThe majority of the packages that you will use are part of the so-called tidyverse package:\n\n#install.packages(\"tidyverse\")\nlibrary(tidyverse)"
  },
  {
    "objectID": "L1-2/lecture1_2.slides.html#r-examples",
    "href": "L1-2/lecture1_2.slides.html#r-examples",
    "title": "Lecture 1-2",
    "section": "R Examples",
    "text": "R Examples\nThe things that we are collecting data from (which could be people, shares, countries, animal species, songs \\(\\dots\\) anything you can collect data on) are called cases or responses. These appear as separate rows in the data matrix. The pieces of information that we use to describe each case are called variables or attributes and these appear in the columns of the data matrix. The \\(x\\)’s, remember, are simply placeholders for values to come. Specifically, the values to come may be numbers, or they may be words. It is perfectly allowable for the first column of \\(x\\)’s to be, for example, the first names of each person, e.g. \\(x_{11}=\\text{Iris}\\). Of course, this will affect the type of analysis we can do later on that variable (for example, it wouldn’t make sense to calculate a mean’ first name).\nExample 1 - Female headed households in SA\nWomxn in Big Data South Africa: Female-Headed Households in South Africa competition\nThe datasets are provided in a .csv file format, test.csv, train.csv, variable_descriptions.csv.\nThe target variable of interest is the percentage of households per ward that are both female-headed and earn an annual income that is below R19,600 (approximately $2,300 USD in 2011).\n\n\n'data.frame':   2822 obs. of  63 variables:\n $ ward             : chr  \"41601001: Ward 1\" \"41601002: Ward 2\" \"41601003: Ward 3\" \"41601004: Ward 4\" ...\n $ total_households : num  1674 1737 2404 1741 1731 ...\n $ total_individuals: num  5888 6735 7273 5734 6657 ...\n $ target           : num  16.8 21.5 10.9 23.1 13.7 ...\n $ dw_00            : num  0.934 0.697 0.811 0.66 0.951 ...\n $ dw_01            : num  0.000846 0.001253 0.004517 0 0.000655 ...\n $ dw_02            : num  0.00549 0.0044 0.00889 0.00613 0.00147 ...\n $ dw_03            : num  0.000676 0 0.003986 0 0.000598 ...\n $ dw_04            : num  0 0.002301 0.007735 0.000813 0.006999 ...\n $ dw_05            : num  0.001372 0.001323 0.000956 0.037245 0.000818 ...\n $ dw_06            : num  0.00575 0.00757 0.00669 0.00526 0.00498 ...\n $ dw_07            : num  0.03147 0.12355 0.02263 0.06891 0.00915 ...\n $ dw_08            : num  0.00808 0.15191 0.1299 0.21879 0.01538 ...\n $ dw_09            : num  0.00282 0.00149 0 0 0.00869 ...\n $ dw_10            : num  0.00143 0.00125 0 0 0 ...\n $ dw_11            : num  0.008224 0.00801 0.00415 0.002947 0.000673 ...\n $ dw_12            : int  0 0 0 0 0 0 0 0 0 0 ...\n $ dw_13            : int  0 0 0 0 0 0 0 0 0 0 ...\n $ psa_00           : num  0.26 0.29 0.186 0.281 0.197 ...\n $ psa_01           : num  0.608 0.55 0.677 0.593 0.518 ...\n $ psa_02           : num  0.000188 0 0.000489 0.000579 0.000989 ...\n $ psa_03           : num  0.01002 0.02134 0.02131 0.00725 0.00515 ...\n $ psa_04           : num  0.122 0.139 0.115 0.118 0.28 ...\n $ stv_00           : num  0.2835 0.1036 0.1658 0.0878 0.346 ...\n $ stv_01           : num  0.717 0.896 0.834 0.912 0.654 ...\n $ car_00           : num  0.274 0.145 0.272 0.128 0.405 ...\n $ car_01           : num  0.726 0.855 0.728 0.872 0.595 ...\n $ lln_00           : num  0.1188 0.0669 0.1 0.0292 0.1336 ...\n $ lln_01           : num  0.881 0.933 0.9 0.971 0.866 ...\n $ lan_00           : num  0.833 0.88 0.566 0.744 0.423 ...\n $ lan_01           : num  0.01234 0.00845 0.01599 0.00653 0.01435 ...\n $ lan_02           : num  0.001923 0.000328 0.001566 0.001188 0.000842 ...\n $ lan_03           : num  0.0509 0.0112 0.1113 0.0864 0.1219 ...\n $ lan_04           : num  0 0.000842 0.004795 0.006735 0.007027 ...\n $ lan_05           : num  0.000564 0.001759 0.002552 0.002308 0.002613 ...\n $ lan_06           : num  0.0761 0.0324 0.1481 0.1032 0.1474 ...\n $ lan_07           : num  0.00637 0.03084 0.13969 0.03828 0.08171 ...\n $ lan_08           : num  0.00366 0.00165 0.00317 0.00308 0.00304 ...\n $ lan_09           : num  0.000375 0.001308 0.000165 0.000582 0.000169 ...\n $ lan_10           : num  0.000372 0.000994 0.000779 0 0.000643 ...\n $ lan_11           : num  0.004943 0 0.001692 0.000197 0.001201 ...\n $ lan_12           : num  0.00272 0.00244 0.00251 0.00744 0.00428 ...\n $ lan_13           : int  0 0 0 0 0 0 0 0 0 0 ...\n $ lan_14           : num  0.006793 0.028061 0.0022 0.000174 0.192272 ...\n $ pg_00            : num  0.357 0.698 0.672 0.728 0.753 ...\n $ pg_01            : num  0.563 0.278 0.154 0.264 0.13 ...\n $ pg_02            : num  0.00426 0.0037 0.00218 0.00181 0.00452 ...\n $ pg_03            : num  0.072996 0.015835 0.167494 0.000956 0.106953 ...\n $ pg_04            : num  0.00212 0.00404 0.00365 0.00539 0.00538 ...\n $ lgt_00           : num  0.919 0.959 0.826 0.986 0.957 ...\n $ pw_00            : num  0.743 0.309 0.323 0.677 0.771 ...\n $ pw_01            : num  0.214 0.577 0.483 0.314 0.195 ...\n $ pw_02            : num  0.01997 0.01895 0.08301 0.00269 0.0097 ...\n $ pw_03            : num  0.00285 0.01457 0.05756 0 0.00486 ...\n $ pw_04            : num  0.007537 0.057127 0.010358 0.000669 0.00129 ...\n $ pw_05            : num  0 0.019092 0.001421 0 0.000673 ...\n $ pw_06            : num  0.01293 0.00413 0.04088 0.00501 0.01763 ...\n $ pw_07            : int  0 0 0 0 0 0 0 0 0 0 ...\n $ pw_08            : int  0 0 0 0 0 0 0 0 0 0 ...\n $ ADM4_PCODE       : chr  \"ZA4161001\" \"ZA4161002\" \"ZA4161003\" \"ZA4161004\" ...\n $ lat              : num  -29.7 -29.1 -29.1 -29.4 -29.4 ...\n $ lon              : num  24.7 24.8 25.1 24.9 25.3 ...\n $ NL               : num  0.292 3.208 0 2.039 0 ...\n\n\n     ward           total_households total_individuals     target     \n Length:2822        Min.   :    1    Min.   :  402     Min.   : 0.00  \n Class :character   1st Qu.: 1779    1st Qu.: 7071     1st Qu.:16.75  \n Mode  :character   Median : 2398    Median : 9367     Median :24.16  \n                    Mean   : 3665    Mean   :12869     Mean   :24.51  \n                    3rd Qu.: 3987    3rd Qu.:14241     3rd Qu.:32.23  \n                    Max.   :39685    Max.   :91717     Max.   :55.53  \n     dw_00            dw_01              dw_02              dw_03          \n Min.   :0.0000   Min.   :0.000000   Min.   :0.000000   Min.   :0.0000000  \n 1st Qu.:0.5942   1st Qu.:0.002895   1st Qu.:0.002407   1st Qu.:0.0000000  \n Median :0.7668   Median :0.010425   Median :0.005762   Median :0.0008066  \n Mean   :0.7122   Mean   :0.092616   Mean   :0.032043   Mean   :0.0060567  \n 3rd Qu.:0.8817   3rd Qu.:0.068209   3rd Qu.:0.027913   3rd Qu.:0.0025383  \n Max.   :0.9950   Max.   :0.931489   Max.   :0.951806   Max.   :0.2642393  \n     dw_04               dw_05               dw_06              dw_07         \n Min.   :0.0000000   Min.   :0.0000000   Min.   :0.000000   Min.   :0.000000  \n 1st Qu.:0.0000000   1st Qu.:0.0000000   1st Qu.:0.002716   1st Qu.:0.004716  \n Median :0.0006069   Median :0.0008654   Median :0.008639   Median :0.016295  \n Mean   :0.0086655   Mean   :0.0062888   Mean   :0.022374   Mean   :0.039296  \n 3rd Qu.:0.0022246   3rd Qu.:0.0030272   3rd Qu.:0.025218   3rd Qu.:0.048731  \n Max.   :0.3920853   Max.   :0.4359115   Max.   :0.412936   Max.   :0.455815  \n     dw_08              dw_09               dw_10               dw_11         \n Min.   :0.000000   Min.   :0.0000000   Min.   :0.0000000   Min.   :0.000000  \n 1st Qu.:0.002888   1st Qu.:0.0002329   1st Qu.:0.0000000   1st Qu.:0.001991  \n Median :0.014991   Median :0.0017552   Median :0.0003909   Median :0.004092  \n Mean   :0.064586   Mean   :0.0068641   Mean   :0.0011121   Mean   :0.007902  \n 3rd Qu.:0.074748   3rd Qu.:0.0065068   3rd Qu.:0.0010425   3rd Qu.:0.007803  \n Max.   :0.798479   Max.   :0.2828433   Max.   :0.0687517   Max.   :1.000000  \n     dw_12       dw_13       psa_00           psa_01        \n Min.   :0   Min.   :0   Min.   :0.0000   Min.   :0.001293  \n 1st Qu.:0   1st Qu.:0   1st Qu.:0.2556   1st Qu.:0.467217  \n Median :0   Median :0   Median :0.3017   Median :0.540874  \n Mean   :0   Mean   :0   Mean   :0.3113   Mean   :0.526568  \n 3rd Qu.:0   3rd Qu.:0   3rd Qu.:0.3712   3rd Qu.:0.586087  \n Max.   :0   Max.   :0   Max.   :0.5616   Max.   :0.852493  \n     psa_02              psa_03            psa_04            stv_00      \n Min.   :0.0000000   Min.   :0.00000   Min.   :0.04279   Min.   :0.0000  \n 1st Qu.:0.0001326   1st Qu.:0.01698   1st Qu.:0.11014   1st Qu.:0.0982  \n Median :0.0003381   Median :0.02705   Median :0.12576   Median :0.1728  \n Mean   :0.0005410   Mean   :0.03369   Mean   :0.12793   Mean   :0.2259  \n 3rd Qu.:0.0006835   3rd Qu.:0.04350   3rd Qu.:0.13973   3rd Qu.:0.3034  \n Max.   :0.0194420   Max.   :0.26738   Max.   :0.99871   Max.   :0.8405  \n     stv_01           car_00           car_01            lln_00       \n Min.   :0.1595   Min.   :0.0000   Min.   :0.04133   Min.   :0.00000  \n 1st Qu.:0.6966   1st Qu.:0.1310   1st Qu.:0.71851   1st Qu.:0.01732  \n Median :0.8272   Median :0.1780   Median :0.82197   Median :0.04014  \n Mean   :0.7741   Mean   :0.2503   Mean   :0.74969   Mean   :0.09764  \n 3rd Qu.:0.9018   3rd Qu.:0.2815   3rd Qu.:0.86902   3rd Qu.:0.12087  \n Max.   :1.0000   Max.   :0.9587   Max.   :1.00000   Max.   :0.76261  \n     lln_01           lan_00             lan_01             lan_02        \n Min.   :0.2374   Min.   :0.000000   Min.   :0.000000   Min.   :0.000000  \n 1st Qu.:0.8791   1st Qu.:0.002842   1st Qu.:0.009433   1st Qu.:0.004081  \n Median :0.9599   Median :0.007914   Median :0.017589   Median :0.008956  \n Mean   :0.9024   Mean   :0.097603   Mean   :0.058684   Mean   :0.029416  \n 3rd Qu.:0.9827   3rd Qu.:0.059327   3rd Qu.:0.036612   3rd Qu.:0.015081  \n Max.   :1.0000   Max.   :0.979246   Max.   :0.939549   Max.   :0.895365  \n     lan_03             lan_04            lan_05             lan_06        \n Min.   :0.000000   Min.   :0.00000   Min.   :0.000000   Min.   :0.000000  \n 1st Qu.:0.001647   1st Qu.:0.01034   1st Qu.:0.001675   1st Qu.:0.002681  \n Median :0.008835   Median :0.05253   Median :0.003986   Median :0.017154  \n Mean   :0.039983   Mean   :0.28432   Mean   :0.116773   Mean   :0.108053  \n 3rd Qu.:0.039564   3rd Qu.:0.56850   3rd Qu.:0.055631   3rd Qu.:0.066745  \n Max.   :0.852927   Max.   :0.98616   Max.   :0.978779   Max.   :0.981207  \n     lan_07             lan_08             lan_09              lan_10         \n Min.   :0.000000   Min.   :0.000000   Min.   :0.0000000   Min.   :0.0000000  \n 1st Qu.:0.003906   1st Qu.:0.001675   1st Qu.:0.0002974   1st Qu.:0.0002999  \n Median :0.008403   Median :0.003045   Median :0.0012675   Median :0.0012002  \n Mean   :0.130673   Mean   :0.004621   Mean   :0.0243186   Mean   :0.0242625  \n 3rd Qu.:0.065157   3rd Qu.:0.005782   3rd Qu.:0.0065378   3rd Qu.:0.0052470  \n Max.   :0.963219   Max.   :0.034234   Max.   :0.9812332   Max.   :0.9828445  \n     lan_11             lan_12             lan_13      lan_14         \n Min.   :0.000000   Min.   :0.000000   Min.   :0   Min.   :0.0000000  \n 1st Qu.:0.000495   1st Qu.:0.002589   1st Qu.:0   1st Qu.:0.0000000  \n Median :0.003261   Median :0.006394   Median :0   Median :0.0001459  \n Mean   :0.053985   Mean   :0.012809   Mean   :0   Mean   :0.0145029  \n 3rd Qu.:0.029783   3rd Qu.:0.013722   3rd Qu.:0   3rd Qu.:0.0121078  \n Max.   :0.991674   Max.   :0.367785   Max.   :0   Max.   :0.9984484  \n     pg_00             pg_01              pg_02               pg_03          \n Min.   :0.01105   Min.   :0.000000   Min.   :0.0000000   Min.   :0.0000000  \n 1st Qu.:0.87528   1st Qu.:0.001015   1st Qu.:0.0008769   1st Qu.:0.0004514  \n Median :0.98975   Median :0.003124   Median :0.0017966   Median :0.0012081  \n Mean   :0.86214   Mean   :0.040938   Mean   :0.0187979   Mean   :0.0744293  \n 3rd Qu.:0.99562   3rd Qu.:0.012582   3rd Qu.:0.0048827   3rd Qu.:0.0418406  \n Max.   :1.00000   Max.   :0.969519   Max.   :0.9395640   Max.   :0.9405628  \n     pg_04               lgt_00             pw_00             pw_01       \n Min.   :0.0000000   Min.   :0.001692   Min.   :0.00000   Min.   :0.0000  \n 1st Qu.:0.0006644   1st Qu.:0.796471   1st Qu.:0.08764   1st Qu.:0.1113  \n Median :0.0016958   Median :0.914061   Median :0.27800   Median :0.3021  \n Mean   :0.0036926   Mean   :0.836432   Mean   :0.35969   Mean   :0.3297  \n 3rd Qu.:0.0041264   3rd Qu.:0.964334   3rd Qu.:0.58295   3rd Qu.:0.5088  \n Max.   :0.3678423   Max.   :1.000000   Max.   :0.99591   Max.   :0.9376  \n     pw_02              pw_03              pw_04               pw_05          \n Min.   :0.000000   Min.   :0.000000   Min.   :0.0000000   Min.   :0.0000000  \n 1st Qu.:0.008673   1st Qu.:0.002099   1st Qu.:0.0007147   1st Qu.:0.0001595  \n Median :0.069065   Median :0.016496   Median :0.0051637   Median :0.0014590  \n Mean   :0.127555   Mean   :0.041589   Mean   :0.0196551   Mean   :0.0110081  \n 3rd Qu.:0.183384   3rd Qu.:0.058626   3rd Qu.:0.0250545   3rd Qu.:0.0094322  \n Max.   :1.000000   Max.   :0.327393   Max.   :0.3067867   Max.   :0.2282606  \n     pw_06              pw_07       pw_08    ADM4_PCODE             lat        \n Min.   :0.000000   Min.   :0   Min.   :0   Length:2822        Min.   :-32.49  \n 1st Qu.:0.005217   1st Qu.:0   1st Qu.:0   Class :character   1st Qu.:-28.57  \n Median :0.025165   Median :0   Median :0   Mode  :character   Median :-26.55  \n Mean   :0.110818   Mean   :0   Mean   :0                      Mean   :-26.88  \n 3rd Qu.:0.116638   3rd Qu.:0   3rd Qu.:0                      3rd Qu.:-25.57  \n Max.   :0.961522   Max.   :0   Max.   :0                      Max.   :-22.33  \n      lon              NL        \n Min.   :16.76   Min.   : 0.000  \n 1st Qu.:27.71   1st Qu.: 3.033  \n Median :28.96   Median : 9.206  \n Mean   :28.67   Mean   :17.438  \n 3rd Qu.:30.44   3rd Qu.:26.891  \n Max.   :32.86   Max.   :63.000  \n\n\nNow we will explore this dataset.\nThe majority of the packages that you will use are part of the so-called tidyverse package:"
  }
]